
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  [{"content":" This section describes how to add KubeSphere cluster nodes.\n The open-source tool KubeKey will be used during the process. For more information about KubeKey, please visit GitHub KubeKey repository.\n     Note     The node addition method described in this section is only applicable to Kubernetes installed through KubeKey. If your Kubernetes is not installed via KubeKey, please refer to Kubernetes Documentation to add nodes.\n    Prerequisites   Contact the KubeSphere Enterprise support team to obtain the KubeSphere Enterprise v4.2.0 installation package.\n  The operating system and version of the cluster nodes must be Ubuntu 16.04, Ubuntu 18.04, Ubuntu 20.04, Ubuntu 22.04, Debian 9, Debian 10, CentOS 7, CentOS Stream, RHEL 7, RHEL 8, SLES 15, or openSUSE Leap 15 or Kylin v10. The operating systems of multiple servers can be different. For support of other operating systems and versions, please contact KubeSphere Enterprise technical support.\n  In a production environment, to ensure the cluster has sufficient computing and storage resources, it is recommended that each cluster node be configured with at least 8 CPU cores, 16 GB of memory, and 200 GB of disk space. In addition, it is recommended to mount an additional 200 GB of disk space in the /var/lib/docker (for Docker) or /var/lib/containerd (for containerd) directory of each cluster node for storing container runtime data.\n  In a production environment, it is recommended to configure high availability for the KubeSphere cluster in advance to avoid service interruption in the event of a single control plane node failure. For more information, please refer to the Configure High Availability.\n  If your cluster nodes cannot connect to the Internet, you also need to prepare a Linux server to create a image registry, and this server must be connected to the KubeSphere Enterprise cluster node network, with at least 100 GB of disk space mounted in the /mnt/registry directory.\n  You should get the installation configuration file config-sample.yaml and transfer it to the cluster node where you will perform this action. For more information, refer to Install {ks_product-en}.\n    Warning       During the process of adding nodes, it is not allowed to modify the original cluster configuration in config-sample.yaml.\n  If you are unable to get the installation configuration file config-sample.yaml, you need to refer to Install {ks_product-en} to recreate the config-sample.yaml file. When recreating the file, make sure that the cluster information in the file matches the current state of the cluster. Otherwise, the uninstallation process may encounter errors.\n           Steps  Transfer the KubeSphere Enterprise installation package to a cluster node and log in to the cluster node.\n  Execute the following command to decompress the installation package and enter the directory generated after decompression (replace \u0026lt;package name\u0026gt; with the actual name of the installation package, and replace \u0026lt;directory\u0026gt; with the directory generated after decompression):\ntar -zxvf \u0026lt;package name\u0026gt;   cd \u0026lt;directory\u0026gt;       Execute the following command to add execution permissions to the KubeKey binary kk:\nsudo chmod +x kk       Transfer the installation configuration file config-sample.yaml to the current directory.\n  Run the following command to edit the installation configuration file config-sample.yaml:\nvi config-sample.yaml       Configure the information of the new node under the hosts parameter in config-sample.yaml.\n    Parameter Description     name\n User-defined server name.\n   address\n The SSH login IP address of the server.\n   internalAddress\n The IP address of the server within the subnet.\n   port\n The SSH port number of the server. This parameter does not need to be set if using the default port 22.\n   user\n The SSH login user name of the server, which must be the root user or another user with sudo permissions. If you use root user, you can not set this parameter.\n   password\n The server’s SSH login password. This parameter does not need to be set if privateKeyPath has been set.\n   privateKeyPath\n The path to the server’s SSH login key. This parameter does not need to be set if password has been set.\n   arch\n The server architecture. If the server’s hardware architecture is Arm64, please set this parameter to arm64, otherwise do not set this parameter. By default, the installation package only supports scenarios where all cluster nodes are x86_64 or arm64 architecture. If the hardware architecture of each cluster node is not exactly the same, please contact the KubeSphere Enterprise technical support team.\n        Warning     Please do not modify the information of the original node. Otherwise, the cluster may encounter errors after adding nodes.\n        Configure the role of the new node in the cluster under the roleGroups parameter in config-sample.yaml.\n    Parameter Description     etcd\n Nodes for installing the etcd database. Set the cluster control plane nodes under this parameter.\n   control-plane\n Cluster control plane nodes. If you have configured high availability for the cluster, you can set multiple control plane nodes.\n   worker\n Cluster worker nodes.\n   registry\n Server used for creating a private image registry. This server is not used as a cluster node. During the installation or upgrade of KubeSphere Enterprise, if the cluster nodes cannot connect to the Internet, you need to set the server used for creating a private image registry under this parameter. Otherwise, you can comment out this parameter.\n        Warning     Please do not modify the role of the original node. Otherwise, the cluster may encounter errors after adding nodes.\n        If a new control plane node is added and the current cluster is not configured for high availability, configure the high availability information under the controlPlaneEndpoint parameter in config-sample.yaml.\n    Parameter Description     internalLoadBalancer\n Type of internal load balancer. If using local load balancer configuration, set this parameter to haproxy. Otherwise, you can comment out this parameter.\n   domain\n Internal access domain for the load balancer. Set this parameter to lb.kubesphere.local.\n   address\n IP address of the load balancer. If using local load balancer configuration, leave this parameter empty. If using a dedicated load balancer, set this parameter to the IP address of the load balancer. If using a generic server as the load balancer, set this parameter to the floating IP address of the load balancer.\n   port\n Port number that the load balancer listens on, which is the port number of the apiserver service. Set this parameter to 6443.\n        Warning       If the current cluster has been configured with high availability, do not modify the high availability information in the config-sample.yaml file. Otherwise, the cluster may encounter errors after adding nodes.\n  If the current cluster uses local load balancing to achieve high availability, you do not need to perform any operations on cluster high availability; if the current cluster uses a load balancer to achieve high availability, you only need to configure the load balancer to listen on port 6443 of all control plane nodes. For more information, see Configure High Availability.\n          Save the configuration file and run the following command to start adding nodes:\n./kk add nodes -f config-sample.yaml -a kubekey-artifact.tar.gz       Run the following command to view the nodes of the current cluster:\nkubectl get node   If it displays the information about the new node, it means the node is added successfully.\n        ","href":"/en/v4.2.0/03-installation-and-upgrade/05-add-and-delete-cluster-nodes/01-add-cluster-nodes/","isSection":null,"linkkey":null,"title":"Add Cluster Nodes"},{"content":" This section provides instructions on how to configure cloud storage devices for KubeSphere in a production environment.\n To configure cloud storage devices, create an API key in the cloud environment and set up KubeSphere using the API key to integrate with the cloud environment through the Container Storage Interface (CSI). When users create volumes in KubeSphere and mount them to pods, KubeSphere will automatically create storage devices in the cloud environment for pods to use. The following steps outline the specific operations using QingCloud as an example. For instructions related to other cloud environments, please refer to the user guide of the respective cloud environment or contact your cloud service provider.\n Prerequisites Get a QingCloud account and ensure that your account balance is sufficient to create the required storage devices. For more information, visit QingCloud Official Website.\n   Steps  Log in to the QingCloud console and click your username in the upper-right corner of the page. Select API Keys from the dropdown list.\n  On the API Keys page, click Create.\n  In the Create API Key dialog, set the name for the key, and click Submit.\n  Download the key file to your local machine from the popup dialog.\n    Warning     KubeSphere will use this private key to connect with the storage devices in the cloud. Please keep this private key file securely to prevent any leakage of user data.\n        Obtain the ID of the API key from the API key list.\n  Log in to the cluster node used for KubeSphere installation and execute the following command to create the storage plugin configuration file:\nvi csi-qingcloud.yaml       Add the following information to the configuration file and save it for future use during KubeSphere installation:\nconfig: qy_access_key_id: \u0026#34;\u0026lt;key ID\u0026gt;\u0026#34; qy_secret_access_key: \u0026#34;\u0026lt;access key\u0026gt;\u0026#34; zone: \u0026#34;\u0026lt;zone ID\u0026gt;\u0026#34; sc: isDefaultClass: true   Replace the following parameters with actual values:\n     Parameter Description     \u0026lt;key ID\u0026gt;\n The ID of the API key.\n   \u0026lt;access key\u0026gt;\n The key text of the API key.\n   \u0026lt;zone ID\u0026gt;\n The availability zone ID of the cloud environment. The zone ID determines the region where the storage devices created by KubeSphere will be located. The ID-value mapping for the availability zones is as follows:\n   sh1a/sh1b: Shanghai Zone 1-A/Shanghai Zone 1-B\n  pek3a/pek3b/pek3c/pek3d: Beijing Zone 3-A/Beijing Zone 3-B/Beijing Zone 3-C/Beijing Zone 3-D\n  gd2a/gd2b: Guangdong Zone 2-A/Guangdong Zone 2-B\n  ap2a: Asia Pacific Zone 2-A\n      The above configuration file includes only the necessary parameters to be set. For other parameters, please refer to QingCloud CSI Configuration.\n        ","href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/01-configure-storage-devices-on-cloud/","isSection":null,"linkkey":null,"title":"Configure Cloud Storage Devices"},{"content":" This section introduces the environment requirements for KubeSphere Enterprise.\n System Requirements     OS Minimum Requirements (per node) Requirements for Production Environment (per node)     Ubuntu 16.04, 18.04, 20.04, 22.04\n CPU: 2 cores, Memory: 4 GB, Disk: 40 GB\n CPU: 8 cores, Memory: 16 GB, Disk: 200 GB\n   Debian 9, Debian 10\n CPU: 2 cores, Memory: 4 GB, Disk: 40 GB\n CPU: 8 cores, Memory: 16 GB, Disk: 200 GB\n   CentOS 7.x, CentOS Stream\n CPU: 2 cores, Memory: 4 GB, Disk: 40 GB\n CPU: 8 cores, Memory: 16 GB, Disk: 200 GB\n   Red Hat Enterprise Linux 7.x, 8.x\n CPU: 2 cores, Memory: 4 GB, Disk: 40 GB\n CPU: 8 cores, Memory: 16 GB, Disk: 200 GB\n   SUSE Linux Enterprise Server 15/openSUSE Leap 15.2\n CPU: 2 cores, Memory: 4 GB, Disk: 40 GB\n CPU: 8 cores, Memory: 16 GB, Disk: 200 GB\n   Kylin v10\n CPU: 2 cores, Memory: 4 GB, Disk: 40 GB\n CPU: 8 cores, Memory: 16 GB, Disk: 200 GB\n      Storage Requirements   A default storage class must be available in the cluster.\nA storage class defines a type of storage volume available for pods. If you did not set up an external persistent storage system when installing KubeSphere Enterprise, KubeSphere Enterprise will use the local storage system of cluster nodes as the persistent storage system and automatically create a corresponding local storage class. If you use an external persistent storage system, you need to install a storage plugin for the KubeSphere Enterprise cluster and create storage classes to define the types of storage volumes available. For information about how to install a storage plugin, contact your storage system provider or refer to Configure External Persistent Storage.\n     Each extension of KubeSphere Enterprise only requires the storage system to provide the capability to create and delete Persistent Volume Claims (PVCs), and does not rely on advanced capabilities such as expansion, cloning, or snapshots.\n     Dependency Requirements     Dependency Kubernetes Version ≥ 1.18 Kubernetes Version \u0026lt; 1.18     socat\n Required\n Optional, but recommended\n   conntrack\n Required\n Optional, but recommended\n   ebtables\n Optional, but recommended\n Optional, but recommended\n   ipset\n Optional, but recommended\n Optional, but recommended\n      Container Runtime Requirements     Supported Container Runtimes Version     Docker\n 20.10.0+\n   containerd\n Latest version\n   CRI-O (Experimental, not fully tested)\n Latest version\n   iSula (Experimental, not fully tested)\n Latest version\n      Network Requirements   Ensure that the DNS addresses in /etc/resolv.conf are accessible; otherwise, it may cause DNS issues in the cluster.\n  If your network configuration uses firewall rules or security groups, make sure that the infrastructure components can communicate with each other through specific ports. It is recommended to disable the firewall.\n  Supported CNI plugins: Calico and Flannel. Other plugins such as Cilium and Kube-OVN are also compatible, but note that they have not been fully tested.\n     Port Requirements Certain ports are used for communication between services. If you have firewall rules in your network configuration, ensure that the infrastructure components can communicate with each other through specific ports. These ports serve as communication endpoints for certain processes or services.\n     Service Protocol Action Start Port End Port Remarks     ssh\n TCP\n allow\n 22\n N/A\n N/A\n   etcd\n TCP\n allow\n 2379\n 2380\n N/A\n   apiserver\n TCP\n allow\n 6443\n N/A\n N/A\n   calico\n TCP\n allow\n 9099\n 9100\n N/A\n   bgp\n TCP\n allow\n 179\n N/A\n N/A\n   nodeport\n TCP\n allow\n 30000\n 32767\n N/A\n   master\n TCP\n allow\n 10250\n 10258\n N/A\n   dns\n TCP\n allow\n 53\n N/A\n N/A\n   dns\n UDP\n allow\n 53\n N/A\n N/A\n   local-registry\n TCP\n allow\n 5000\n N/A\n Required for offline environments\n   local-apt\n TCP\n allow\n 5080\n N/A\n Required for offline environments\n   rpcbind\n TCP\n allow\n 111\n N/A\n Required when using NFS\n   ipip\n IPENCAP / IPIP\n allow\n N/A\n N/A\n Calico requires IPIP protocol\n   metrics-server\n TCP\n allow\n 8443\n N/A\n N/A\n      Extension Support Matrix KubeSphere Enterprise supports Kubernetes v1.23~1.32 by default. This section details the Kubernetes versions supported by each extension of KubeSphere Enterprise. To avoid compatibility issues during use, please use the supported Kubernetes versions.\n     Extension Extension Version Supported Kubernetes Versions Supported Architectures     ks-core\n 4.1.x\n 1.23~1.32\n amd64、arm64\n   DevOps\n 1.2.x\n 1.23~1.32\n amd64、arm64\n   Image Builder\n 0.1.0\n 1.23~1.32\n amd64、arm64\n   App Store Management\n 2.1.x\n 1.23~1.32\n amd64、arm64\n   Service Mesh\n 1.0.x\n 1.22~1.32\n amd64、arm64\n   Application Management for Cluster Federation\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Observability Platform\n 2.0.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Data Pipeline\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Logging\n 1.2.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Events\n 1.2.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Auditing\n 1.2.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Monitoring\n 1.2.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Alerting\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Global Monitoring\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Events Alerting\n 1.2.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Notification\n 2.6.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Network Observability\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   WizTelemetry Tracing\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   OpenSearch\n 2.11.1\n 1.23~1.32\n amd64、arm64\n   Grafana for WizTelemetry\n 10.4.x\n 1.23~1.32\n amd64、arm64\n   Grafana Loki for WizTelemetry\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   Grafana Alloy for WizTelemetry\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   Grafana Tempo for WizTelemetry\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   Network\n 1.2.x\n 1.23~1.32\n amd64、arm64\n   Gateway\n 1.1.x\n 1.22~1.32\n amd64、arm64\n   Ingress Utils\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   Storage\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   RadonDB DMP\n 2.2.x\n 1.23~1.32\n amd64、arm64\n   EdgeWize\n 3.1.x\n 1.23-1.23\n amd64、arm64\n   Spring Cloud\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   Multi-Cluster Agent Connection\n 1.1.x\n 1.23~1.32\n amd64、arm64\n   Gatekeeper\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   OAuth2-Proxy\n 7.6.x\n 1.23~1.32\n amd64、arm64\n   NVIDIA GPU Operator\n 23.9.x\n 1.23~1.32\n amd64、arm64\n   Cert Manager\n 1.0.x\n 1.23~1.32\n amd64、arm64\n   Metrics Server\n 0.7.x\n 1.23~1.32\n amd64、arm64\n      ","href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/01-supported-k8s/","isSection":null,"linkkey":null,"title":"Environment Requirements"},{"content":" This section explains how to install Kubernetes and KubeSphere Enterprise in an Internet-accessible environment.\n The installation process will use the open-source tool KubeKey. For more information about KubeKey, please visit the GitHub KubeKey repository.\n Prerequisites   Prepare at least 1 Linux server as a cluster node. In a production environment, to ensure high availability of the cluster, it is recommended to prepare at least 5 Linux servers, with 3 servers as control plane nodes and 2 servers as worker nodes. If you are installing KubeSphere Enterprise on multiple Linux servers, make sure all servers belong to the same subnet.\n  The operating system and version of the cluster nodes must be Ubuntu 16.04, Ubuntu 18.04, Ubuntu 20.04, Ubuntu 22.04, Debian 9, Debian 10, CentOS 7, CentOS Stream, RHEL 7, RHEL 8, SLES 15, or openSUSE Leap 15 or Kylin v10. The operating systems of multiple servers can be different. For support of other operating systems and versions, please contact KubeSphere Enterprise technical support.\n  In a production environment, to ensure the cluster has sufficient computing and storage resources, it is recommended that each cluster node be configured with at least 8 CPU cores, 16 GB of memory, and 200 GB of disk space. In addition, it is recommended to mount an additional 200 GB of disk space in the /var/lib/docker (for Docker) or /var/lib/containerd (for containerd) directory of each cluster node for storing container runtime data.\n  In a production environment, it is recommended to configure high availability for the KubeSphere cluster in advance to avoid service interruption in the event of a single control plane node failure. For more information, please refer to the Configure High Availability.\n    Note     If you plan to have multiple control plane nodes, be sure to configure high availability for the cluster in advance.\n        By default, KubeSphere uses the local disk space of the cluster nodes as persistent storage. In a production environment, it is recommended to configure an external storage system as persistent storage in advance. For more information, please refer to the Configure External Persistent Storage.\n  If the cluster nodes do not have a container runtime installed, the installation tool KubeKey will automatically install Docker as the container runtime for each cluster node during the installation process. You can also manually install containerd, CRI-O, or iSula as the container runtime in advance.\n    Note     CRI-O and iSula have not been fully tested for compatibility with KubeSphere, and there may be unknown issues.\n        Make sure the DNS server addresses configured in the /etc/resolv.conf file on all cluster nodes are available. Otherwise, the KubeSphere cluster may encounter domain name resolution issues.\n  Make sure you can use the sudo, curl, and openssl commands on all cluster nodes.\n  Make sure the time is synchronized on all cluster nodes.\n     Configure Firewall Rules KubeSphere requires specific ports and protocols for communication between services. If your infrastructure environment has enabled a firewall, you need to open the required ports and protocols in the firewall settings. If your infrastructure environment does not have a firewall enabled, you can skip this step.\n The following table lists the ports and protocols that need to be opened in the firewall.\n     Service Protocol Start Port End Port Remarks     ssh\n TCP\n 22\n     etcd\n TCP\n 2379\n 2380\n    apiserver\n TCP\n 6443\n     calico\n TCP\n 9099\n 9100\n    bgp\n TCP\n 179\n     nodeport\n TCP\n 30000\n 32767\n    master\n TCP\n 10250\n 10258\n    dns\n TCP\n 53\n     dns\n UDP\n 53\n     metrics-server\n TCP\n 8443\n     local-registry\n TCP\n 5000\n  Required for offline environments\n   local-apt\n TCP\n 5080\n  Required for offline environments\n   rpcbind\n TCP\n 111\n  Required when using NFS as persistent storage\n   ipip\n IPENCAP/IPIP\n   Required when using Calico\n      Install Dependencies You need to install socat, conntrack, ebtables, and ipset for all cluster nodes. If the above dependencies already exist on each cluster node, you can skip this step.\n On Ubuntu systems, run the following command to install the dependencies on the servers:\n sudo apt install socat conntrack ebtables ipset -y   If the cluster nodes use other operating systems, replace apt with the corresponding package management tool for the operating system.\n   Create a Kubernetes Cluster  If you are accessing GitHub/Googleapis from a restricted location, please log in to any cluster node and run the following command to set the download region:\nexport KKZONE=cn       Run the following command to download the latest version of KubeKey:\ncurl -sfL https://get-kk.kubesphere.io | sh -   After the download is complete, a KubeKey binary file kk will be generated in the current directory.\n     Note     If the cluster node used to perform the operations cannot connect to the internet, you can manually {{\u0026lt; adoclink \u0026#34;/_custom-en/installationAndUpgrade/https:/github.com/kubesphere/kubekey/releases\u0026#34; \u0026gt;}}download KubeKey{{\u0026lt; /adoclink \u0026gt;}} on a device with internet access and then transfer it to the cluster node.\n        Add execute permission to the KubeKey binary file kk:\nsudo chmod +x kk       Run the following command to create the installation configuration file config-sample.yaml:\n./kk create config --with-kubernetes \u0026lt;Kubernetes version\u0026gt;   Replace \u0026lt;Kubernetes version\u0026gt; with the actual required version, for example v1.27.4. KubeSphere Enterprise natively supports Kubernetes v1.23~1.32.\n After the command executes, it will generate the installation configuration file config-sample.yaml.\n     Note     After KubeSphere Enterprise is installed, please do not delete config-sample.yaml. This file will still be used for subsequent operations such as adding nodes. If this file is missing, you will need to recreate it.\n        Run the following command to edit the installation configuration file config-sample.yaml:\nvi config-sample.yaml   The following is a part of the configuration file sample. For a complete example, please refer to this file.\n apiVersion: kubekey.kubesphere.io/v1alpha2 kind: Cluster metadata: name: sample spec: hosts: - {name: controlplane1, address: 192.168.0.2, internalAddress: 192.168.0.2, port: 23, user: ubuntu, password: Testing123, arch: arm64} # For arm64 nodes, please add the parameter arch: arm64 - {name: controlplane2, address: 192.168.0.3, internalAddress: 192.168.0.3, user: ubuntu, privateKeyPath: \u0026#34;~/.ssh/id_rsa\u0026#34;} - {name: worker1, address: 192.168.0.4, internalAddress: 192.168.0.4, user: ubuntu, password: Testing123} - {name: worker2, address: 192.168.0.5, internalAddress: 192.168.0.5, user: ubuntu, password: Testing123} - {name: registry, address: 192.168.0.6, internalAddress: 192.168.0.6, user: ubuntu, password: Testing123} roleGroups: etcd: - controlplane1 - controlplane2 control-plane: - controlplane1 - controlplane2 worker: - worker1 - worker2 # If you want to use kk to automatically deploy the image registry, please set up the registry (it is recommended that the image registry and cluster nodes be deployed separately to reduce mutual influence) registry: - registry controlPlaneEndpoint: internalLoadbalancer: haproxy # If you need to deploy a high availability cluster and no load balancer is available, you can enable this parameter to perform load balancing within the cluster. domain: lb.kubesphere.local address: \u0026#34;\u0026#34; port: 6443 kubernetes: version: v1.23.15 clusterName: cluster.local network: plugin: calico kubePodsCIDR: 10.233.64.0/18 kubeServiceCIDR: 10.233.0.0/18 ## multus support. https://github.com/k8snetworkplumbingwg/multus-cni enableMultusCNI: false registry: # If you want to use kk to deploy harbor, you can set this parameter to harbor. If you do not set this parameter and you need to use kk to deploy the image registry, docker registry will be deployed by default. # Harbor does not support arm64. This parameter does not need to be configured when deploying in an arm64 environment. type: harbor # If you use kk to deploy harbor or other registries that require authentication, you need to set the auths of the corresponding registries. If you use kk to deploy the default docker registry, you do not need to configure the auths parameter. # Note: If you use kk to deploy harbor, please set the auths parameter after creating the harbor project. auths: \u0026#34;dockerhub.kubekey.local\u0026#34;: username: admin # harbor default username password: Harbor12345 # harbor default password plainHTTP: false # If the registry uses http, please set this parameter to true privateRegistry: \u0026#34;dockerhub.kubekey.local/kse\u0026#34; # Set the private registry address used during cluster deployment. registryMirrors: [] insecureRegistries: [] addons: []       Set the information of each server under the spec:hosts parameter in config-sample.yaml.\n    Parameter Description     name\n User-defined server name.\n   address\n The SSH login IP address of the server.\n   internalAddress\n The IP address of the server within the subnet.\n   port\n The SSH port number of the server. This parameter does not need to be set if using the default port 22.\n   user\n The SSH login user name of the server, which must be the root user or another user with sudo permissions. If you use root user, you can not set this parameter.\n   password\n The server’s SSH login password. This parameter does not need to be set if privateKeyPath has been set.\n   privateKeyPath\n The path to the server’s SSH login key. This parameter does not need to be set if password has been set.\n   arch\n The server architecture. If the server’s hardware architecture is Arm64, please set this parameter to arm64, otherwise do not set this parameter. By default, the installation package only supports scenarios where all cluster nodes are x86_64 or arm64 architecture. If the hardware architecture of each cluster node is not exactly the same, please contact the KubeSphere Enterprise technical support team.\n        Set the server’s role under the spec:roleGroups parameter in config-sample.yaml.\n    Parameter Description     etcd\n Nodes for installing the etcd database. Set the cluster control plane nodes under this parameter.\n   control-plane\n Cluster control plane nodes. If you have configured high availability for the cluster, you can set multiple control plane nodes.\n   worker\n Cluster worker nodes.\n   registry\n Server used for creating a private image registry. This server is not used as a cluster node. During the installation or upgrade of KubeSphere Enterprise, if the cluster nodes cannot connect to the Internet, you need to set the server used for creating a private image registry under this parameter. Otherwise, you can comment out this parameter.\n        If you have multiple control plane nodes, set high availability information under the spec:controlPlaneEndpoint parameter in config-sample.yaml.\n    Parameter Description     internalLoadBalancer\n Type of internal load balancer. If using local load balancer configuration, set this parameter to haproxy. Otherwise, you can comment out this parameter.\n   domain\n Internal access domain for the load balancer. Set this parameter to lb.kubesphere.local.\n   address\n IP address of the load balancer. If using local load balancer configuration, leave this parameter empty. If using a dedicated load balancer, set this parameter to the IP address of the load balancer. If using a generic server as the load balancer, set this parameter to the floating IP address of the load balancer.\n   port\n Port number that the load balancer listens on, which is the port number of the apiserver service. Set this parameter to 6443.\n        If you need to use external persistence storage, set the external persistence storage information under the spec:addons parameter in config-sample.yaml.\n  If using a cloud storage device, set the following parameters under spec:addons (replace \u0026lt;configuration file path\u0026gt; with the actual path of the storage plugin configuration file):\n- name: csi-qingcloud namespace: kube-system sources: chart: name: csi-qingcloud repo: https://charts.kubesphere.io/test valuesFile: \u0026lt;configuration file path\u0026gt;       If using NeonSAN storage, set the following parameters under spec:addons (replace \u0026lt;configuration file path\u0026gt; with the actual path of the storage plugin configuration file):\n- name: csi-neonsan namespace: kube-system sources: chart: name: csi-neonsan repo: https://charts.kubesphere.io/test valuesFile: \u0026lt;configuration file path\u0026gt;       If using an NFS storage system, set the following parameters under spec:addons (replace \u0026lt;configuration file path\u0026gt; with the actual path of the storage plugin configuration file):\n- name: nfs-client namespace: kube-system sources: chart: name: nfs-client-provisioner repo: https://charts.kubesphere.io/main valuesFile: \u0026lt;configuration file path\u0026gt;            Run the following command to create the Kubernetes cluster:\n./kk create cluster -f config-sample.yaml       Note     If you want to use OpenEBS LocalPV, you can add the --with-local-storage parameter after the command. If you want to integrate with other storage solutions, configure the relevant storage plugins under the spec:addons parameter in config-sample.yaml  or install them after the Kubernetes deployment.\n    If you see the following information, it means that the Kubernetes cluster is successfully created.\n Pipeline[CreateclusterPipeline] execute successfully          Install KubeSphere Enterprise KubeSphere Core (ks-core) is the core component of KubeSphere Enterprise, providing the fundamental runtime environment for extensions. After KubeSphere Core is installed, you can access the KubeSphere Enterprise web console.\n  On the cluster nodes, run the following command to install KubeSphere Core.\nchart=oci://hub.kubesphere.com.cn/kse/ks-core version=1.2.1 helm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --debug --wait --version $version --reset-values   If you see the following information, it means that ks-core installation is successful:\n NOTES: Thank you for choosing KubeSphere Helm Chart. Please be patient and wait for several seconds for the KubeSphere deployment to complete. 1. Wait for Deployment Completion Confirm that all KubeSphere components are running by executing the following command: kubectl get pods -n kubesphere-system 2. Access the KubeSphere Console Once the deployment is complete, you can access the KubeSphere console using the following URL: http://192.168.6.10:30880 3. Login to KubeSphere Console Use the following credentials to log in: Account: admin Password: P@88w0rd NOTE: It is highly recommended to change the default password immediately after the first login. For additional information and details, please visit https://kubesphere.io.       From the successful information, retrieve the Console, Account, and Password parameters to obtain the IP address of the KubeSphere Enterprise web console, the admin username, and the admin password. Use a web browser to log in to the KubeSphere Enterprise web console.\n    Note     Depending on your network environment, you may need to configure traffic forwarding rules and open port 30880 in the firewall.\n      By now, the KubeSphere Enterprise web console only provides the core functionalities of KubeSphere Enterprise. To access more features, you need to install extensions from the Extensions Center.\nBefore using KubeSphere Enterprise and its extensions, please first activate KubeSphere Enterprise and extensions.\n      ","href":"/en/v4.2.0/03-installation-and-upgrade/02-install-kubesphere/01-online-install-kubernetes-and-kubesphere/","isSection":null,"linkkey":null,"title":"Online Installation of Kubernetes and KubeSphere Enterprise"},{"content":"This section describes the preparation work required before installing KubeSphere Enterprise.\n ","href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/","isSection":null,"linkkey":null,"title":"Preparation"},{"content":" This section explains how to uninstall KubeSphere Enterprise. The Kubernetes running in the current cluster will not be uninstalled.\n     Warning       Although Kubernetes will not be uninstalled, the uninstallation may still cause service interruption if the business running in the current cluster relies on the functionalities provided by KubeSphere.\n  The uninstallation is irreversible. Please proceed with caution.\n      Prerequisites To avoid data loss, please back up all important data in advance.\n   Uninstall Extensions  Log in to any cluster node and run the following command to view all installed extensions in the cluster.\nkubectl get installplan     Uninstall specific extensions or all extensions.\n  Uninstall specific extensions.\nkubectl delete installplan {InstallPlan Name}       Note     You can get the InstallPlan Name of the extension according to the command from the first step.\n    If the following information is displayed, it indicates that the uninstallation of the extension is successful.\n installplan.kubesphere.io \u0026#34;{InstallPlan Name}\u0026#34; deleted       Uninstall all extensions.\nkubectl delete installplan --all   The output should be as follows:\n installplan.kubesphere.io \u0026#34;devops\u0026#34; deleted installplan.kubesphere.io \u0026#34;dmp\u0026#34; deleted installplan.kubesphere.io \u0026#34;gatekeeper\u0026#34; deleted installplan.kubesphere.io \u0026#34;gateway\u0026#34; deleted installplan.kubesphere.io \u0026#34;kubefed\u0026#34; deleted installplan.kubesphere.io \u0026#34;metrics-server\u0026#34; deleted installplan.kubesphere.io \u0026#34;network\u0026#34; deleted installplan.kubesphere.io \u0026#34;openpitrix\u0026#34; deleted installplan.kubesphere.io \u0026#34;opensearch\u0026#34; deleted installplan.kubesphere.io \u0026#34;springcloud\u0026#34; deleted installplan.kubesphere.io \u0026#34;storage-utils\u0026#34; deleted installplan.kubesphere.io \u0026#34;tower\u0026#34; deleted installplan.kubesphere.io \u0026#34;vector\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-alerting\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-auditing\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-events\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-logging\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-monitoring\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-notification\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-telemetry\u0026#34; deleted installplan.kubesphere.io \u0026#34;whizard-telemetry-ruler\u0026#34; deleted   Run the following command again. If it outputs \u0026#34;No resources found\u0026#34;, it means that all extensions have been uninstalled.\n kubectl get installplan             Uninstall ks-core Before uninstalling ks-core, make sure that all extensions in the cluster have been uninstalled, that is, executing the kubectl get installplan command shows \u0026#34;No resources found\u0026#34;.\n  Run the following command to uninstall ks-core.\nhelm del -n kubesphere-system ks-core     Run the following command. If the output is empty (as shown below), it means that KubeSphere Enterprise has been successfully uninstalled.\nroot@xxx:~# helm list -n kubesphere-system NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION          ","href":"/en/v4.2.0/03-installation-and-upgrade/04-uninstall-kubesphere/01-uninstall-kubesphere-only/","isSection":null,"linkkey":null,"title":"Uninstall KubeSphere Enterprise Only"},{"content":"","firstChild":{"href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/02-configure-high-availability/02-configure-k8s-high-availability/","title":"Configure Kubernetes High Availability"},"href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/02-configure-high-availability/","isSection":null,"linkkey":null,"title":"Configure High Availability"},{"content":" This section explains how to configure multiple control plane nodes for high availability in a production environment for the KubeSphere cluster. This ensures that the cluster services remain operational even if a single control plane node fails. If your KubeSphere cluster does not require high availability, you can skip this section.\n This section explains the following methods for configuring high availability:\n   Local Load Balancer Configuration: You can install HAProxy on the worker nodes during the KubeSphere installation process using the KubeKey tool. HAProxy will act as a reverse proxy for the control plane nodes, and the Kubernetes components on the worker nodes will connect to the control plane nodes through HAProxy. This method requires additional health check mechanisms and may reduce efficiency compared to other methods, but can be used in scenarios without a dedicated load balancer and with a limited number of servers.\n  Dedicated Load Balancer: You can use a load balancer provided by your cloud environment as a reverse proxy for the control plane nodes. This method requires deploying the KubeSphere cluster in a cloud environment that offers a dedicated load balancer.\n  Generic Servers as Load Balancers: You can install Keepalived and HAProxy on Linux servers outside the cluster nodes to act as load balancers. This method requires at least two additional Linux servers.\n   Local Load Balancer Configuration To use HAProxy for high availability, you need to configure the following parameters in the installation configuration file config-sample.yaml during the installation of KubeSphere Enterprise:\n spec: controlPlaneEndpoint: internalLoadbalancer: haproxy domain: lb.kubesphere.local address: \u0026#34;\u0026#34; port: 6443   KubeKey will automatically install HAProxy on the worker nodes and complete the high availability configuration, requiring no additional actions. For more information, please refer to Install {ks_product-en}.\n   Dedicated Load Balancer To achieve high availability using a dedicated load balancer provided by your cloud environment, you need to perform the following steps within your cloud environment:\n  Create a load balancer with a minimum of two replicas in your cloud environment.\n  Configure the load balancer to listen on port 6443 of each control plane node in the KubeSphere cluster.\n  Obtain the IP address of the load balancer for future use during the installation of KubeSphere Enterprise.\n   For specific instructions, please refer to the user guide of your cloud environment or contact your cloud service provider.\n   Generic Servers as Load Balancers The following describes how to configure a generic server as a load balancer using Keepalived and HAProxy.\n Prerequisites   You need to prepare two Linux servers that belong to the same private network as the cluster nodes as load balancers.\n  You need to prepare a Virtual IP address (VIP) to serve as the floating IP address for the two load balancer servers. This address should not be used by any other devices or components to avoid address conflicts.\n    Configure High Availability  Log in to the server that will be used as the load balancer and execute the following command to install HAProxy and Keepalived (the example assumes Ubuntu as the operating system; please replace apt with the corresponding package manager for other operating systems):\napt install keepalived haproxy psmisc -y       Execute the following command to edit the HAProxy configuration file:\nvi /etc/haproxy/haproxy.cfg       Add the following information to the HAProxy configuration file and save the file (replace \u0026lt;IP address\u0026gt; with the private IP addresses of the control plane nodes in the KubeSphere cluster):\nglobal log /dev/log local0 warning chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon stats socket /var/lib/haproxy/stats defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:6443 mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server kube-apiserver-1 \u0026lt;IP address\u0026gt;:6443 check server kube-apiserver-2 \u0026lt;IP address\u0026gt;:6443 check server kube-apiserver-3 \u0026lt;IP address\u0026gt;:6443 check       Execute the following command to restart HAProxy:\nsystemctl restart haproxy       Execute the following command to set HAProxy to run automatically on startup:\nsystemctl enable haproxy       Execute the following command to edit the Keepalived configuration file:\nvi /etc/keepalived/keepalived.conf       Add the following information to the Keepalived configuration file and save the file:\nglobal_defs { notification_email { } router_id LVS_DEVEL vrrp_skip_check_adv_addr vrrp_garp_interval 0 vrrp_gna_interval 0 } vrrp_script chk_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight 2 } vrrp_instance haproxy-vip { state BACKUP priority 100 interface \u0026lt;NIC\u0026gt; virtual_router_id 60 advert_int 1 authentication { auth_type PASS auth_pass 1111 } unicast_src_ip \u0026lt;source IP address\u0026gt; unicast_peer { \u0026lt;peer IP address\u0026gt; } virtual_ipaddress { \u0026lt;floating IP address\u0026gt; } track_script { chk_haproxy } }   Replace the following parameters with actual values:\n     Parameter Description     \u0026lt;NIC\u0026gt;\n The network interface card (NIC) of the current load balancer.\n   \u0026lt;source IP address\u0026gt;\n The IP address of the current load balancer.\n   \u0026lt;peer IP address\u0026gt;\n The IP address of the other load balancer.\n   \u0026lt;floating IP address\u0026gt;\n The virtual IP address used as the floating IP address.\n        Execute the following command to restart Keepalived:\nsystemctl restart keepalived       Execute the following command to set Keepalived to run automatically on startup:\nsystemctl enable keepalived       Repeat the above steps to install and configure HAProxy and Keepalived on the other load balancer server.\n  Record the floating IP address for future use during the installation of KubeSphere Enterprise.\n    Verify High Availability  Log in to the first load balancer server and execute the following command to check the floating IP address:\nip a s   If the system’s high availability is functioning properly, the configured floating IP address will be displayed in the command output. For example, in the following command output, inet 172.16.0.10/24 scope global secondary eth0 indicates that the floating IP address is bound to the eth0 network interface:\n 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 73334sec preferred_lft 73334sec inet 172.16.0.10/24 scope global secondary eth0 valid_lft forever preferred_lft forever inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute valid_lft forever preferred_lft forever       Execute the following command to simulate a failure on the current load balancer server:\nsystemctl stop haproxy       Execute the following command again to check the floating IP address:\nip a s   If the system’s high availability is functioning properly, the command output will no longer display the floating IP address, as shown in the following command output:\n 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:27:38:c8 brd ff:ff:ff:ff:ff:ff inet 172.16.0.2/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 72802sec preferred_lft 72802sec inet6 fe80::510e:f96:98b2:af40/64 scope link noprefixroute valid_lft forever preferred_lft forever       Log in to the other load balancer server and execute the following command to view the floating IP address:\nip a s   If the system’s high availability is functioning properly, the configured floating IP address will be displayed in the command output. For example, in the following command output, inet 172.16.0.10/24 scope global secondary eth0 indicates that the floating IP address is bound to the eth0 network interface:\n 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:9e:3f:51:ba brd ff:ff:ff:ff:ff:ff inet 172.16.0.3/24 brd 172.16.0.255 scope global noprefixroute dynamic eth0 valid_lft 72690sec preferred_lft 72690sec inet 172.16.0.10/24 scope global secondary eth0 valid_lft forever preferred_lft forever inet6 fe80::f67c:bd4f:d6d5:1d9b/64 scope link noprefixroute valid_lft forever preferred_lft forever       Execute the following command on the first load balancer server to restore the running of HAProxy:\nsystemctl start haproxy         ","href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/02-configure-high-availability/02-configure-k8s-high-availability/","isSection":null,"linkkey":null,"title":"Configure Kubernetes High Availability"},{"content":"This section introduces how to install Kubernetes and KubeSphere Enterprise.\n ","href":"/en/v4.2.0/03-installation-and-upgrade/02-install-kubesphere/","isSection":null,"linkkey":null,"title":"Install KubeSphere Enterprise"},{"content":" This section describes how to delete KubeSphere Enterprise cluster nodes.\n The open-source tool KubeKey will be used during the process. For more information about KubeKey, please visit GitHub KubeKey repository.\n     Note     The node deletion method described in this section is only applicable to Kubernetes installed through KubeKey. If your Kubernetes is not installed via KubeKey, please refer to Kubernetes documentation to delete nodes.\n        Warning       Do not delete control plane nodes, otherwise the cluster may encounter errors.\n  Please ensure that after node deletion, the remaining resources in the cluster are still sufficient to run existing services. Otherwise, service interruption may occur after the node is deleted.\n      Prerequisites   Contact the KubeSphere Enterprise support team to obtain the KubeSphere Enterprise v4.2.0 installation package.\n  You should get the installation configuration file config-sample.yaml and transfer it to the cluster node where you will perform this action. For more information, refer to Install {ks_product-en}.\n    Warning     If you are unable to get the installation configuration file config-sample.yaml, you need to refer to Install {ks_product-en} to recreate the config-sample.yaml file. When recreating the file, make sure that the cluster information in the file matches the current state of the cluster. Otherwise, the uninstallation process may encounter errors.\n         Steps  Transfer the KubeSphere Enterprise installation package to a cluster node and log in to the cluster node.\n  Execute the following command to decompress the installation package and enter the directory generated after decompression (replace \u0026lt;package name\u0026gt; with the actual name of the installation package, and replace \u0026lt;directory\u0026gt; with the directory generated after decompression):\ntar -zxvf \u0026lt;package name\u0026gt;   cd \u0026lt;directory\u0026gt;       Execute the following command to add execution permissions to the KubeKey binary kk:\nsudo chmod +x kk       Run the following command to get the name of the node that needs to be deleted:\nkubectl get node       Run the following command to evict the pods running on the node that needs to be deleted to other nodes (replace \u0026lt;node name\u0026gt; with the name of the node that needs to be deleted):\nkubectl drain \u0026lt;node name\u0026gt;       Transfer the installation configuration file config-sample.yaml to the current directory.\n  Run the following command to start deleting nodes:\n./kk delete node \u0026lt;node name\u0026gt; -f config-sample.yaml       Run the following command to view the current cluster nodes:\nkubectl get node   If the deleted node is not displayed, it means the node is deleted successfully.\n        ","href":"/en/v4.2.0/03-installation-and-upgrade/05-add-and-delete-cluster-nodes/02-delete-cluster-nodes/","isSection":null,"linkkey":null,"title":"Remove Cluster Nodes"},{"content":" This section explains how to uninstall Kubernetes and KubeSphere Enterprise.\n     Note     The uninstallation method for Kubernetes described in this section only applies to Kubernetes installed using KubeKey. If your Kubernetes is not installed through KubeKey, please refer to the Kubernetes Documentation for uninstallation instructions.\n        Warning     The uninstallation will cause business interruption to the KubeSphere cluster and cannot be undone. Please proceed with caution.\n    Prerequisites   You should get the installation configuration file config-sample.yaml and transfer it to the cluster nodes where you will perform the uninstallation. For more information, refer to Install {ks_product-en}.\n    Warning     If you are unable to get the installation configuration file config-sample.yaml, you need to refer to Install {ks_product-en} to recreate the config-sample.yaml file. When recreating the file, make sure that the cluster information in the file matches the current state of the cluster. Otherwise, the uninstallation process may encounter errors.\n       To avoid data loss, please back up all important data in advance.\n     Steps  Transfer the KubeSphere Enterprise installation package to a cluster node and log in to the cluster node.\n  Execute the following command to decompress the installation package and enter the directory generated after decompression (replace \u0026lt;package name\u0026gt; with the actual name of the installation package, and replace \u0026lt;directory\u0026gt; with the directory generated after decompression):\ntar -zxvf \u0026lt;package name\u0026gt;   cd \u0026lt;directory\u0026gt;       Execute the following command to add execution permissions to the KubeKey binary kk:\nsudo chmod +x kk       Transfer the installation configuration file config-sample.yaml to the current directory.\n  Run the following command to uninstall KubeSphere Enterprise.\n./kk delete cluster -f config-sample.yaml   If you see the following message, it means the uninstallation is successful:\n Pipeline[DeleteClusterPipeline] execute successful          ","href":"/en/v4.2.0/03-installation-and-upgrade/04-uninstall-kubesphere/02-uninstall-kubernetes-and-kubesphere/","isSection":null,"linkkey":null,"title":"Uninstall Kubernetes and KubeSphere Enterprise"},{"content":" After logging in to the KubeSphere Enterprise web console, you need to import a license to activate KubeSphere Enterprise and extensions to access all features.\n     Note     If you installed KubeSphere Enterprise offline, please contact delivery specialists directly to get the license.\n    Step 1: Purchase and obtain license For online installations of KubeSphere Enterprise, visit {ks_product-en} official website, click Contact Us, and inquire about purchasing method for KubeSphere Enterprise and extensions licenses via WeChat or Slack.\n \n   Step 2: Import license After obtaining the license, go to Component Dock \u0026gt; Platform Settings \u0026gt; Licenses to add licenses for KubeSphere Enterprise and extensions.\n   Step 3: Verify activation If the license page shows KubeSphere Enterprise and extensions as Authorized, it indicates successful activation and you can start using KubeSphere Enterprise.\n   ","href":"/en/v4.2.0/03-installation-and-upgrade/02-install-kubesphere/03-activate-kse/","isSection":null,"linkkey":null,"title":"Activate KubeSphere Enterprise"},{"content":"This guide introduces how to install and uninstall KubeSphere Enterprise, as well as how to add and remove cluster nodes.\n ","href":"/en/v4.2.0/03-installation-and-upgrade/","isSection":null,"linkkey":null,"title":"Installation Guide"},{"content":" This section explains how to upgrade from KubeSphere Enterprise v4.1.2 or v4.1.3 to v4.2.0 when Internet access is available.\n Prerequisites   Ensure current KubeSphere Enterprise version is v4.1.2 or v4.1.3.\n  Ensure current Kubernetes version is v1.23.x to v1.32.x.\n  For extensions with special configurations, back up extension configurations by downloading files from the \u0026#34;Extension Config\u0026#34; dialog.\n   To avoid data loss, please back up all important data in advance.\n     Attention  If you customized the nodeShell image in v4.1.x, specify the nodeShell image in the upgrade configuration file kse-v4.2.0-host-custom-values.yaml as shown below:\nExample configuration in v4.1.x:\n nodeShell: image: registry: \u0026#34;\u0026#34; repository: kubesphereio/kubectl tag: \u0026#34;v1.27.12\u0026#34; pullPolicy: IfNotPresent   Configure in kse-v4.2.0-host-custom-values.yaml as:\n terminal: kubectl: enabled: true image: registry: \u0026#34;\u0026#34; repository: kubesphereio/kubectl tag: \u0026#34;v1.33.1\u0026#34; pullPolicy: IfNotPresent node: enabled: true image: registry: \u0026#34;\u0026#34; repository: kubesphereio/kubectl tag: \u0026#34;v1.33.1\u0026#34; pullPolicy: IfNotPresent pod: enabled: true uploadFileLimit: \u0026#34;100Mi\u0026#34; uploadFileEnabled: true downloadFileEnabled: true       Starting from v4.1.3, cluster role and host cluster name configurations in ks-core chart have changed. When upgrading from v4.1.2, configure as shown below (v4.1.3 unaffected):\nmulticluster: # Cluster role name role: \u0026#34;\u0026#34; # Host cluster name (priority: direct specification \u0026gt; reading from kubesphere-config \u0026gt; default name host) hostClusterName: \u0026#34;\u0026#34;     Starting from v4.1.3, the following parameter is deprecated. Remove it or set to false in kse-v4.2.0-host-custom-values.yaml:\nupgrade: enabled: false        Upgrade KubeSphere Enterprise KubeSphere Enterprise v4.1 and later versions use Helm charts for ks-core upgrades.\n Upgrade Host Cluster  Verify the current cluster is the target host cluster for upgrade:\nkubectl get node -o wide     KubeSphere Enterprise v4.1.3 removed kse-extensions-publish and integrated it into the ks-core chart. For v4.1.2 to v4.2.0 upgrades, you should patch extension resources created by KubeSphere Enterprise previous versions:\n    Note     This step only applies to v4.1.2 → v4.2.0 upgrades. Skip for v4.1.3 → v4.2.0.\n     Create the extension-resources-patch.sh script to handle conflicts from kse-extension-publish created resources:\nvi extension-resources-patch.sh   Paste and save:\n #!/bin/bash # Resolve resource template conflicts kubectl -n kubesphere-system label deploy extensions-museum app.kubernetes.io/managed-by=Helm kubectl -n kubesphere-system annotate deploy extensions-museum meta.helm.sh/release-name=ks-core kubectl -n kubesphere-system annotate deploy extensions-museum meta.helm.sh/release-namespace=kubesphere-system kubectl -n kubesphere-system label service extensions-museum app.kubernetes.io/managed-by=Helm kubectl -n kubesphere-system annotate service extensions-museum meta.helm.sh/release-name=ks-core kubectl -n kubesphere-system annotate service extensions-museum meta.helm.sh/release-namespace=kubesphere-system kubectl -n kubesphere-system label secret extensions-museum-certs app.kubernetes.io/managed-by=Helm kubectl -n kubesphere-system annotate secret extensions-museum-certs meta.helm.sh/release-name=ks-core kubectl -n kubesphere-system annotate secret extensions-museum-certs meta.helm.sh/release-namespace=kubesphere-system # Resolve repository reference conflicts for item in $(kubectl get extensionversions.kubesphere.io -o jsonpath=\u0026#34;{.items[*].metadata.name}\u0026#34;); do kubectl patch extensionversions.kubesphere.io $item --type merge -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;repository\u0026#34;:\u0026#34;extensions-museum\u0026#34;}}\u0026#39; kubectl label extensionversions.kubesphere.io $item kubesphere.io/repository-ref=extensions-museum done for item in $(kubectl get extensions.kubesphere.io -o jsonpath=\u0026#34;{.items[*].metadata.name}\u0026#34;); do kubectl label extensions.kubesphere.io $item kubesphere.io/repository-ref=extensions-museum done       Execute the script:\nbash extension-resources-patch.sh        Check current ks-core configuration:\nhelm get values -n kubesphere-system ks-core       Create the upgrade configuration file with the following content:\nIf the previous step shows parameters beyond image registry, image version, cloud and upgrade, include them in this configuration file.\n cat \u0026lt;\u0026lt;EOF \u0026gt; kse-v4.2.0-host-custom-values.yaml # Specify image registry for ks-core and extensions # Modify according to actual environment global: imageRegistry: registry.cn-beijing.aliyuncs.com extension: imageRegistry: \u0026#34;\u0026#34; # Cluster role parameter changed from role to multicluster.role multicluster: role: host # Enable HA for ks-core components (ks-apiserver, ks-controller-manager, ks-console) # Configure according to cluster status ha: enabled: false # Enable Redis HA (required for ks-apiserver HA) # If set to false, single-replica Redis will be deployed in kubesphere-system redisHA: enabled: false EOF       Run the following command to start the upgrade.\nchart=oci://hub.kubesphere.com.cn/kse/ks-core version=1.2.1 helm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version -f kse-v4.2.0-host-custom-values.yaml --wait --debug       Verify the host cluster upgrade status.\nRun the following command. The pods should be in Running state as shown below.\n root@xxx:~# kubectl get pod -n kubesphere-system NAME READY STATUS RESTARTS AGE extensions-museum-85f846dbbd-6xtst 1/1 Running 0 16h helm-install-ks-console-embed-tnwmf7-wcfjf 0/1 Completed 0 16h ks-apiserver-7f875b8654-zvhrd 1/1 Running 0 16h ks-console-997fc9658-dnrqr 1/1 Running 0 16h ks-console-embed-775f757548-9vd2s 1/1 Running 0 16h ks-controller-manager-5f69675d48-qnxv7 1/1 Running 0 16h       Access the KubeSphere Enterprise v4.2.0 web console using the original IP address, administrator username and password.\n  Check whether all functions and data in the host cluster are working properly.\n    Upgrade Member Cluster The member cluster upgrade process is similar to the host cluster, with special attention to member-specific parameters.\n  Verify the current member cluster is the target for upgrade:\nkubectl get node -o wide     Check member cluster’s ks-core configuration:\nhelm get values -n kubesphere-system ks-core       Get the member cluster’s jwtSecret:\nkubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v \u0026#34;apiVersion\u0026#34; | grep jwtSecret     Create the upgrade configuration file with the following content:\nIf the previous step shows parameters beyond image registry, image version, cloud and upgrade, include them in this configuration file.\n cat \u0026lt;\u0026lt;EOF \u0026gt; kse-v4.2.0-member-custom-values.yaml # Specify image registry for ks-core # Modify according to actual environment global: imageRegistry: registry.cn-beijing.aliyuncs.com # Replace with member cluster\u0026#39;s jwtSecret value authentication: issuer: jwtSecret: \u0026lt;REPLACE_ME\u0026gt; # Cluster role parameter changed from role to multicluster.role multicluster: role: member EOF       Run the upgrade command:\nchart=oci://hub.kubesphere.com.cn/kse/ks-core version=1.2.1 helm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version -f kse-v4.2.0-member-custom-values.yaml --wait --debug       Verify the member cluster upgrade status.\nRun the following command; the ks-agent pod should be in the running state as shown below.\n root@xxx:~# kubectl get pod -n kubesphere-system NAME READY STATUS RESTARTS AGE ks-agent-5dc5b57977-4x6mf 2/2 Running 0 59m       If you added custom configurations (e.g., --set a=b) in the above upgrade command, you need to add the custom configurations for the member cluster in the web console.\nMethod: On the Cluster Management page, click  on the right side of the member cluster you want to edit, and then select Edit Configuration from the dropdown list. In the pop-up window, enter a: b.\n     Note     If you did not include custom configurations in the upgrade command, you do not need to add cluster configurations in the web console.\n          Upgrade Extensions Upgrade the required extensions in the Extensions Center. For extensions with special configurations that have been backed up, modify the configurations before upgrading.\n  In the Extensions Center, click the extension name to enter its details page.\n  Click  below the extension icon and select Extension Config.\n  Clear the Custom Config field, enter the customized configurations, then click OK.\n  Click  below the extension icon again and select Update.\n  In the Extension Update dialog box, click Start Update and wait for the upgrade to complete.\n   At this point, KubeSphere Enterprise has completed all upgrades.\n Appendix: Upgradeable Extensions List\n     Extension KSE v4.1.2 KSE v4.1.3 KSE v4.2.0     alloy\n Not included\n 1.0.0\n 1.0.0\n   cert-manager\n 1.0.0\n 1.0.0\n 1.0.0\n   devops\n 1.1.1\n 1.1.2\n 1.2.1\n   s2i\n Not included\n Not included\n 0.1.0\n   dmp\n 2.1.3\n 2.1.4\n 2.2.0\n   edgewize\n Not included\n Not included\n 3.1.0\n   gatekeeper\n 1.0.1\n 1.0.3\n 1.0.3\n   gateway\n 1.0.2\n 1.0.3\n 1.1.1\n   grafana\n 10.4.3\n 10.4.14\n 10.4.14\n   ingress-utils\n 1.0.0\n 1.0.0\n 1.0.1\n   kubeedge\n 1.13.1\n 1.13.1\n Removed (migrated to EdgeWize)\n   kubefed\n 1.0.0\n 1.0.0\n 1.1.0\n   loki\n 1.0.2\n 1.0.2\n 1.0.2\n   metrics-server\n 0.7.0\n 0.7.0\n 0.7.2\n   network\n 1.1.0\n 1.1.0\n 1.2.1\n   nvidia-gpu-operator\n 23.9.2\n 23.9.2\n 23.9.2\n   oauth2-proxy\n 7.6.2\n 7.6.2\n 7.6.2\n   openpitrix\n 2.0.1\n 2.0.2\n 2.1.0\n   opensearch\n 2.11.1\n 2.11.1\n 2.11.1\n   s2ibuilder\n Not included\n Not included\n 0.1.0\n   servicemesh\n 1.0.0\n 1.0.1\n 1.0.2\n   springcloud\n 1.0.1\n 1.0.1\n 1.0.2\n   storage-utils\n 1.0.0\n 1.0.0\n 1.0.1\n   tempo\n Not included\n Not included\n 1.0.0\n   tower\n 1.0.0\n 1.0.0\n 1.1.0\n   vector\n 1.0.4\n 1.0.4\n 1.0.4\n   whizard-alerting\n 1.0.3\n 1.0.4\n 1.0.5\n   whizard-auditing\n 1.2.0\n 1.2.1\n 1.2.1\n   whizard-events\n 1.2.0\n 1.2.1\n 1.2.2\n   whizard-logging\n 1.2.2\n 1.2.3\n 1.2.3\n   whizard-monitoring-pro\n Not included\n Not included\n 1.0.0\n   whizard-monitoring\n 1.1.0\n 1.1.0\n 1.2.0\n   whizard-notification\n 2.5.9\n 2.6.0\n 2.6.1\n   whizard-telemetry-ruler\n 1.2.0\n 1.2.0\n 1.2.0\n   whizard-telemetry\n 1.2.2\n 1.3.0\n 2.0.0\n   wiztelemetry-bpfconductor\n Not included\n Not included\n 1.0.3\n   wiztelemetry-tracing\n Not included\n Not included\n 1.0.3\n       ","href":"/en/v4.2.0/03-installation-and-upgrade/03-upgrade-kubesphere/03-online-upgrade-kubephere-from-4.1.x/","isSection":null,"linkkey":null,"title":"Online Upgrade from v4.1.x to v4.2.0"},{"content":"This section only describes how to upgrade from KubeSphere Enterprise v4.1.2 or v4.1.3 to v4.2.0. Direct upgrades from v3.5 to v4.2.0 are not supported.\n To upgrade from v3.5 to v4.2.0, please contact KubeSphere Enterprise delivery specialists to get the upgrade documentation and upgrade to v4.1.2.\n ","href":"/en/v4.2.0/03-installation-and-upgrade/03-upgrade-kubesphere/","isSection":null,"linkkey":null,"title":"Upgrade KubeSphere Enterprise"},{"content":" This section explains how to configure an external persistent storage system for the KubeSphere cluster in a production environment. A persistent storage system is used to create volumes for storing application data in KubeSphere. If an external persistent storage system is not configured, KubeSphere will default to using the local storage system of the cluster nodes. If your KubeSphere cluster does not require external persistent storage, you can skip this section.\n     Note     The configuration of external persistent storage for KubeSphere is only supported in scenarios where Kubernetes and KubeSphere are installed together. If you install KubeSphere on an existing Kubernetes cluster, KubeSphere will use the existing persistent storage configuration of the Kubernetes cluster.\n    How to Install a Storage System When installing KubeSphere Enterprise, different storage systems can be installed as plugins. KubeKey creates a configuration file for the cluster (default is config-sample.yaml) that contains all necessary parameters defining different resources (including storage plugins). To enable KubeKey to install these storage systems as expected, you must provide KubeKey with the necessary configurations for these storage systems.\n Typically, there are two methods to apply configurations of storage systems that KubeKey will install:\n  Enter the necessary parameters directly under the addons field in config-sample.yaml.\n  Create a separate configuration file for the plugin listing all necessary parameters, and provide the file path in config-sample.yaml so KubeKey can reference this path during installation.\n   For more information, see Addons.\n   Default Storage Class   KubeKey supports installing different storage plugins and storage classes. Regardless of which storage system you want to install, you can specify whether to set it as the default storage class in its configuration file.\n  If you plan to install multiple storage plugins, only one of them can be set as the default storage class. Otherwise, KubeKey won’t be able to determine which storage class to use.\n     ","href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/","isSection":null,"linkkey":null,"title":"Configure External Persistent Storage"},{"content":" This section explains how to configure high availability for KubeSphere.\n     Note     KubeSphere high availability relies on high availability of Kubernetes control plane nodes, so ensure Kubernetes is deployed in HA mode first.\n    1. High Availability Architecture Overview KubeSphere supports high availability deployment, which can be enabled via ha.enabled.\n In HA mode, Redis supports two deployment approaches:\n  Redis standalone mode\n  Redis high availability mode (Redis HA)\n     2. Version Compatibility KubeSphere HA configuration applies to KubeSphere Enterprise v4.1.2 and later versions.\n   3. KubeSphere HA Configuration 3.1 Enable HA Mode Create a values.yaml file with the following configuration:\n ha: enabled: true      4. Redis Configuration Choose either Redis standalone or Redis HA mode based on requirements, then add the corresponding configuration to the values.yaml file.\n 4.1 Redis Standalone Mode Suitable for small clusters with simple configuration and lower resource consumption.\n redis: port: 6379 replicaCount: 1 image: repository: kubesphereio/redis tag: 7.2.4-alpine pullPolicy: IfNotPresent persistentVolume: enabled: true size: 2Gi    4.2 Redis HA Mode Designed for production environments with full high availability.\n redisHA: enabled: true redis: port: 6379 image: repository: kubesphereio/redis tag: 7.2.4-alpine pullPolicy: IfNotPresent persistentVolume: enabled: true size: 2Gi    4.3 Redis HA Advanced Configuration redisHA: enabled: true # Redis node configuration redis: port: 6379 # Persistence configuration persistentVolume: enabled: true size: 2Gi # Node affinity tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule - key: node-role.kubernetes.io/control-plane effect: NoSchedule - key: CriticalAddonsOnly operator: Exists # HA configuration hardAntiAffinity: false additionalAffinities: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: node-role.kubernetes.io/control-plane operator: In values: - \u0026#34;\u0026#34; # HAProxy configuration haproxy: servicePort: 6379 containerPort: 6379 image: repository: kubesphereio/haproxy tag: 2.9.6-alpine pullPolicy: IfNotPresent      5. HA Deployment When installing or upgrading KubeSphere Enterprise, append -f values.yaml to your command.\n     Note     The following commands are examples only. Always append -f values.yaml to your actual installation/upgrade command.\n    # Installation helm install -n kubesphere-system --create-namespace ks-core oci://hub.kubesphere.com.cn/kse/ks-core --version 1.1.0 -f values.yaml # Upgrade helm upgrade -n kubesphere-system ks-core oci://hub.kubesphere.com.cn/kse/ks-core --version 1.1.0 -f values.yaml     6. Configuration Notes 6.1 Redis Standalone Mode   Designed for small clusters\n  Uses single Redis instance\n  Supports basic failover\n  Simple configuration with low resource overhead\n    6.2 Redis HA Mode   Recommended for production\n  Uses Redis cluster\n  Provides full HA capabilities\n  Supports automatic failover\n  Data persistence\n  Load balancing\n      7. Optional Configurations JWT Signing Key Configuration In high availability environments, configure a custom SignKey to ensure all replicas use the same JWT signing key.\n  Generate an RSA private key:\nopenssl genrsa -out private_key.pem 2048     View the Base64-encoded key content:\ncat private_key.pem | base64 -w 0     Edit KubeSphere configuration:\nkubectl -n kubesphere-system edit cm kubesphere-config   Add or replace the following field under authentication.issuer:\n signKeyData: \u0026lt;Base64-encoded-private-key\u0026gt;       Restart KubeSphere components:\nkubectl -n kubesphere-system rollout restart deploy ks-apiserver ks-controller-manager     Verify configuration: Access http://\u0026lt;ks-console-address\u0026gt;/oauth/keys multiple times in your browser and verify the data consistency across all replicas.\n      ","href":"/en/v4.2.0/03-installation-and-upgrade/02-install-kubesphere/04-configure-ks-high-availability/","isSection":null,"linkkey":null,"title":"Configure KubeSphere High Availability"},{"content":" NeonSAN is an enterprise-grade distributed block storage system developed by QingCloud. NeonSAN CSI is a storage plugin provided by the NeonSAN team for Kubernetes, which enables dynamic provisioning of persistent storage volumes on the Kubernetes platform.\n This section explains how to configure NeonSAN CSI for KubeSphere clusters in a production environment.\n Prerequisites   NeonSAN v2.2.0 or above has been successfully deployed, and each node in the container cluster is connected to NeonSAN by the installed QBD. For specific operations, please consult KubeSphere technical support.\n  Kubernetes v1.16 or above has been installed.\n  Helm has been installed on the master node of the container cluster. Helm 3 is used as an example in this section.\n     Steps Online Installation of NeonSAN CSI Online installation is applicable to container cluster that has access to the Internet.\n  Execute the following command to add the Helm repository, such as https://charts.kubesphere.io/test.\n$ helm repo add ks-test https://charts.kubesphere.io/test \u0026#34;ks-test\u0026#34; has been added to your repositories       Execute the following command to check if the repository has been added successfully.\n$ helm repo list NAME URL ks-test https://charts.kubesphere.io/test       Execute the following command to update the repository’s chart list.\n$ helm repo update       Execute the following command to search for the installation package of NeonSAN CSI in the repository.\n$ helm search repo neonsan NAME CHART VERSION APP VERSION DESCRIPTION ks-test/csi-neonsan 1.2.2 1.2.0 A Helm chart for NeonSAN CSI Driver       Check the version of qbd installed on the master node.\n$ qbd -v Package Version: 2.2.0-336092c-202202101432-ubuntu2004 Loaded Module Version: 2.2.0-336092c-202209010306-testlangchaor01n01 NeonSAN Static Library Version: 3.0.0-092498bf NeonSAN Protocol Version: 1       Execute the following command to install NeonSAN CSI. Set the parameter driver.repository based on the qbd version installed on the master node. For example, if the qbd version is 2.2.0, the parameter in the command should be driver.repository=\u0026#34;csiplugin/csi-neonsan-qbd2.2.0\u0026#34;.\n$ helm install csi-neonsan ks-test/csi-neonsan --namespace kube-system --set driver.tag=\u0026#34;v1.2.3\u0026#34; --set sc.rep_count=2 --set driver.repository=\u0026#34;csiplugin/csi-neonsan-qbd2.2.0\u0026#34; NAME: csi-neonsan LAST DEPLOYED: Fri Nov 20 10:28:32 2020 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None       Execute the following command to check if NeonSAN CSI is installed successfully.\n$ helm list -n kube-system NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION csi-neonsan kube-system 1 2020-11-20 10:28:32.240990384 +0800 CST deployed csi-neonsan-1.2.2 1.2.0       Check if the pod is in Running state.\n$ kubectl get pod -n kube-system | grep csi-neonsan kube-system csi-neonsan-controller-75dc5cbcff-6gk54 5/5 Running 0 38s kube-system csi-neonsan-node-8vd8l 2/2 Running 0 38s kube-system csi-neonsan-node-dxk2z 2/2 Running 0 38s kube-system csi-neonsan-node-mp2b2 2/2 Running 0 38s       Check if all NeonSAN CSI components are running normally.\n  When READY equals AVAILABLE, the csi-neonsan-controller is normal.\n$ kubectl -n kube-system get deployments.apps csi-neonsan-controller NAME READY UP-TO-DATE AVAILABLE AGE csi-neonsan-controller 1/1 1 1 66m       When DESIRED equals READY and AVAILABLE, the csi-neonsan-node is normal.\n$ kubectl -n kube-system get daemonsets.apps csi-neonsan-node NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE csi-neonsan-node 3 3 3 3 3 \u0026lt;none\u0026gt; 66m            Check if the storage class is installed.\n$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE csi-neonsan neonsan.csi.qingstor.com Delete Immediate true 2m56s       View the storage pool used by the storage class. This storage pool must exist in NeonSAN, otherwise you cannot create storage volumes using this storage class.\n$ kubectl get storageclass csi-neonsan -o yaml | grep pool_name pool_name: kube       Log in to the NeonSAN server and check if the storage pool used by the storage class exists. If not, create the storage pool using the neonsan create_pool command.\n$ neonsan list_pool -pool kube -detail Pool Count: 1 +----------+------+---------------------------+ | ID | NAME | CREATED TIME | +----------+------+---------------------------+ | 33554432 | kube | 2020-08-07T14:53:52+08:00 | +----------+------+---------------------------+         Offline Installation of NeonSAN CSI Offline installation is applicable to container clusters that cannot access the Internet.\n  On your local machine, download the NeonSAN CSI installation package and copy the package to the cluster Master node.\n$ helm repo add ks-test https://charts.kubesphere.io/test \u0026#34;ks-test\u0026#34; has been added to your repositories $ helm pull ks-test/csi-neonsan $ ls -l csi-neonsan*.tgz -rw-r--r--. 1 root root 5196 Nov 20 13:13 csi-neonsan-1.2.2.tgz       Execute the following command to view all image files required by NeonSAN CSI.\n$ helm show values ks-test/csi-neonsan driver: repository: csiplugin/csi-neonsan tag: v1.2.0 node: repository: csiplugin/csi-neonsan-ubuntu tag: v1.2.0 provisioner: repository: csiplugin/csi-provisioner tag: v1.5.0 volumeNamePrefix: pvc attacher: repository: csiplugin/csi-attacher tag: v2.1.1 resizer: repository: csiplugin/csi-resizer tag: v0.4.0 snapshotter: repository: csiplugin/csi-snapshotter tag: v2.0.1 registrar: repository: csiplugin/csi-node-driver-registrar tag: v1.2.0       Use Docker to download all images locally and package them, or upload them to an internal repository (such as harbor).\ndocker pull csiplugin/csi-neonsan:v1.2.0 docker pull csiplugin/csi-neonsan-ubuntu:v1.2.0 docker pull csiplugin/csi-provisioner:v1.5.0 docker pull csiplugin/csi-attacher:v2.1.1 docker pull csiplugin/csi-resizer:v0.4.0 docker pull csiplugin/csi-snapshotter:v2.0.1 docker pull csiplugin/csi-node-driver-registrar:v1.2.0     docker save csiplugin/csi-neonsan:v1.2.0 \\ csiplugin/csi-neonsan-ubuntu:v1.2.0 \\ csiplugin/csi-provisioner:v1.5.0 \\ csiplugin/csi-attacher:v2.1.1 \\ csiplugin/csi-resizer:v0.4.0 \\ csiplugin/csi-snapshotter:v2.0.1 \\ csiplugin/csi-node-driver-registrar:v1.2.0 \\ -o neonsan-csi-images.tar       Execute the following commands to upload the image package to a directory on all nodes in the cluster, such as the /tmp directory, extract it, and install it.\n$ scp neonsan-csi-images.tar user@node1:/tmp/ scp neonsan-csi-images.tar user@node2:/tmp/ ...   $ tar -xvf /tmp/neonsan-csi-images.tar -C /       Execute the following commands to check if the installation is complete. If you see all the NeonSAN CSI images in the list, the installation is successful.\n$ docker images       Refer to the steps 8 - 12 of online installation for post-installation checks.\nAfter successfully installing NeonSAN CSI, you can view it in the Storage section on the KubeSphere web console.\n         ","href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/04-configure-neosan-csi/","isSection":null,"linkkey":null,"title":"Configure NeonSAN CSI"},{"content":" This section describes how to perform an offline upgrade from KubeSphere Enterprise v4.1.2 to KubeSphere Enterprise v4.2.0 without Internet access.\n Prerequisites   Contact the KubeSphere Enterprise support team to get the KubeSphere Enterprise v4.2.0 installation package and license.\n  Ensure current KubeSphere Enterprise version is v4.1.2 or v4.1.3.\n  Ensure current Kubernetes version is v1.23.x to v1.32.x.\n  For extensions with special configurations, back up extension configurations by downloading files from the \u0026#34;Extension Config\u0026#34; dialog.\n   To avoid data loss, please back up all important data in advance.\n     Attention  If you customized the nodeShell image in v4.1.x, specify the nodeShell image in the upgrade configuration file kse-v4.2.0-host-custom-values.yaml as shown below:\nExample configuration in v4.1.x:\n nodeShell: image: registry: \u0026#34;\u0026#34; repository: kubesphereio/kubectl tag: \u0026#34;v1.27.12\u0026#34; pullPolicy: IfNotPresent   Configure in kse-v4.2.0-host-custom-values.yaml as:\n terminal: kubectl: enabled: true image: registry: \u0026#34;\u0026#34; repository: kubesphereio/kubectl tag: \u0026#34;v1.33.1\u0026#34; pullPolicy: IfNotPresent node: enabled: true image: registry: \u0026#34;\u0026#34; repository: kubesphereio/kubectl tag: \u0026#34;v1.33.1\u0026#34; pullPolicy: IfNotPresent pod: enabled: true uploadFileLimit: \u0026#34;100Mi\u0026#34; uploadFileEnabled: true downloadFileEnabled: true       Starting from v4.1.3, cluster role and host cluster name configurations in ks-core chart have changed. When upgrading from v4.1.2, configure as shown below (v4.1.3 unaffected):\nmulticluster: # Cluster role name role: \u0026#34;\u0026#34; # Host cluster name (priority: direct specification \u0026gt; reading from kubesphere-config \u0026gt; default name host) hostClusterName: \u0026#34;\u0026#34;     Starting from v4.1.3, the following parameter is deprecated. Remove it or set to false in kse-v4.2.0-host-custom-values.yaml:\nupgrade: enabled: false        Preparation 1. Unpack the Installation Package  Transfer the KubeSphere Enterprise installation package to the cluster node to be upgraded and log in to that node.\n  Run the following command to unpack the v4.2.0 installation package (replace \u0026lt;package name\u0026gt; with the actual package name).\ntar -zxvf \u0026lt;package name\u0026gt;       Attention     Ensure that the directory has sufficient available space, preferably greater than 100 GB.\n        Enter the directory generated after unpacking the installation package.\ncd \u0026lt;directory\u0026gt;         2. Push Images  In the v4.2.0 directory, edit the config-sample.yaml file created during the installation and modify the registry section within it.\nprivateRegistry: \u0026#34;dockerhub.kubekey.local/kse\u0026#34; # Replace with the actual image registry address auths: \u0026#34;dockerhub.kubekey.local\u0026#34;: # Replace with the actual image registry address username: admin # Replace with the username of the image registry password: Harbor12345 # Replace with the password of the image registry skipTLSVerify: true plainHTTP: true # Set this parameter to true if the image registry uses http       Push the images to the image registry.\n./kk artifact images push -f config-sample.yaml -a kubekey-artifact.tar.gz         Upgrade KubeSphere Enterprise KubeSphere Enterprise v4.1 and later versions use the helm chart method to upgrade ks-core.\n Upgrade Host Cluster  Verify the current cluster is the target host cluster for upgrade:\nkubectl get node -o wide     KubeSphere Enterprise v4.1.3 removed kse-extensions-publish and integrated it into the ks-core chart. For v4.1.2 to v4.2.0 upgrades, you should patch extension resources created by KubeSphere Enterprise previous versions:\n    Note     This step only applies to v4.1.2 → v4.2.0 upgrades. Skip for v4.1.3 → v4.2.0.\n    # Use the extension-resources-patch.sh script located in the tools directory of the installation package bash tools/extension-resources-patch.sh     Check the current cluster’s ks-core configuration.\nhelm get value -n kubesphere-system ks-core       Create the upgrade configuration file with the following content:\nIf there are other parameters besides the image registry, image version, cloud, and upgrade in the previous step, please add them to the following configuration file.\n cat \u0026lt;\u0026lt;EOF \u0026gt; kse-v4.2.0-host-custom-values.yaml # Specify image registry for ks-core and extensions # Modify according to actual environment global: imageRegistry: dockerhub.kubekey.local/kse extension: imageRegistry: dockerhub.kubekey.local/kse # Cluster role parameter changed from role to multicluster.role multicluster: role: host # Enable HA for ks-core components (ks-apiserver, ks-controller-manager, ks-console) # Configure according to cluster status ha: enabled: false # Enable Redis HA (required for ks-apiserver HA) # If set to false, single-replica Redis will be deployed in kubesphere-system redisHA: enabled: false EOF       Run the following command to start the upgrade.\nhelm upgrade --install ks-core charts/ks-core -n kubesphere-system -f kse-v4.2.0-host-custom-values.yaml --wait --debug       Attention     Replace charts/ks-core with the actual path of the ks-core chart in the current environment.\n        Verify the host cluster upgrade status.\nRun the following command. The pods should be in Running state as shown below.\n root@xxx:~# kubectl get pod -n kubesphere-system NAME READY STATUS RESTARTS AGE extensions-museum-85f846dbbd-6xtst 1/1 Running 0 16h helm-install-ks-console-embed-tnwmf7-wcfjf 0/1 Completed 0 16h ks-apiserver-7f875b8654-zvhrd 1/1 Running 0 16h ks-console-997fc9658-dnrqr 1/1 Running 0 16h ks-console-embed-775f757548-9vd2s 1/1 Running 0 16h ks-controller-manager-5f69675d48-qnxv7 1/1 Running 0 16h       Access the KubeSphere Enterprise v4.2.0 web console using the original IP address, administrator username and password.\n  Check whether all functions and data in the host cluster are working properly.\n    Upgrade Member Cluster The member cluster upgrade process is similar to the host cluster, with special attention to member-specific parameters.\n  Verify the current member cluster is the target for upgrade:\nkubectl get node -o wide     Check member cluster’s ks-core configuration:\nhelm get values -n kubesphere-system ks-core       Get the member cluster’s jwtSecret:\nkubectl -n kubesphere-system get cm kubesphere-config -o yaml | grep -v \u0026#34;apiVersion\u0026#34; | grep jwtSecret     Create the upgrade configuration file with the following content:\nIf the previous step shows parameters beyond image registry, image version, cloud and upgrade, include them in this configuration file.\n cat \u0026lt;\u0026lt;EOF \u0026gt; kse-v4.2.0-member-custom-values.yaml # Specify image registry for ks-core # Modify according to actual environment global: imageRegistry: dockerhub.kubekey.local/kse # Replace with member cluster\u0026#39;s jwtSecret value authentication: issuer: jwtSecret: \u0026lt;REPLACE_ME\u0026gt; # Cluster role parameter changed from role to multicluster.role multicluster: role: member EOF       Run the upgrade command:\nhelm upgrade --install ks-core charts/ks-core -n kubesphere-system -f kse-v4.2.0-member-custom-values.yaml --wait --debug       Attention     Replace charts/ks-core with the actual path of the ks-core chart in the current environment.\n        Verify the member cluster upgrade status.\nRun the following command; the ks-agent pod should be in the running state as shown below.\n root@xxx:~# kubectl get pod -n kubesphere-system NAME READY STATUS RESTARTS AGE ks-agent-5dc5b57977-4x6mf 2/2 Running 0 59m       If you added custom configurations (e.g., --set a=b) in the above upgrade command, you need to add the custom configurations for the member cluster in the web console.\nMethod: On the Cluster Management page, click  on the right side of the member cluster you want to edit, and then select Edit Configuration from the dropdown list. In the pop-up window, enter a: b.\n     Note     If you did not include custom configurations in the upgrade command, you do not need to add cluster configurations in the web console.\n          Upgrade Extensions Upgrade the required extensions in the Extensions Center. For extensions with special configurations that have been backed up, modify the configurations before upgrading.\n  In the Extensions Center, click the extension name to enter its details page.\n  Click  below the extension icon and select Extension Config.\n  Clear the Custom Config field, enter the customized configurations, then click OK.\n  Click  below the extension icon again and select Update.\n  In the Extension Update dialog box, click Start Update and wait for the upgrade to complete.\n   At this point, KubeSphere Enterprise has completed all upgrades.\n Appendix: Upgradeable Extensions List\n     Extension KSE v4.1.2 KSE v4.1.3 KSE v4.2.0     alloy\n Not included\n 1.0.0\n 1.0.0\n   cert-manager\n 1.0.0\n 1.0.0\n 1.0.0\n   devops\n 1.1.1\n 1.1.2\n 1.2.1\n   s2i\n Not included\n Not included\n 0.1.0\n   dmp\n 2.1.3\n 2.1.4\n 2.2.0\n   edgewize\n Not included\n Not included\n 3.1.0\n   gatekeeper\n 1.0.1\n 1.0.3\n 1.0.3\n   gateway\n 1.0.2\n 1.0.3\n 1.1.1\n   grafana\n 10.4.3\n 10.4.14\n 10.4.14\n   ingress-utils\n 1.0.0\n 1.0.0\n 1.0.1\n   kubeedge\n 1.13.1\n 1.13.1\n Removed (migrated to EdgeWize)\n   kubefed\n 1.0.0\n 1.0.0\n 1.1.0\n   loki\n 1.0.2\n 1.0.2\n 1.0.2\n   metrics-server\n 0.7.0\n 0.7.0\n 0.7.2\n   network\n 1.1.0\n 1.1.0\n 1.2.1\n   nvidia-gpu-operator\n 23.9.2\n 23.9.2\n 23.9.2\n   oauth2-proxy\n 7.6.2\n 7.6.2\n 7.6.2\n   openpitrix\n 2.0.1\n 2.0.2\n 2.1.0\n   opensearch\n 2.11.1\n 2.11.1\n 2.11.1\n   s2ibuilder\n Not included\n Not included\n 0.1.0\n   servicemesh\n 1.0.0\n 1.0.1\n 1.0.2\n   springcloud\n 1.0.1\n 1.0.1\n 1.0.2\n   storage-utils\n 1.0.0\n 1.0.0\n 1.0.1\n   tempo\n Not included\n Not included\n 1.0.0\n   tower\n 1.0.0\n 1.0.0\n 1.1.0\n   vector\n 1.0.4\n 1.0.4\n 1.0.4\n   whizard-alerting\n 1.0.3\n 1.0.4\n 1.0.5\n   whizard-auditing\n 1.2.0\n 1.2.1\n 1.2.1\n   whizard-events\n 1.2.0\n 1.2.1\n 1.2.2\n   whizard-logging\n 1.2.2\n 1.2.3\n 1.2.3\n   whizard-monitoring-pro\n Not included\n Not included\n 1.0.0\n   whizard-monitoring\n 1.1.0\n 1.1.0\n 1.2.0\n   whizard-notification\n 2.5.9\n 2.6.0\n 2.6.1\n   whizard-telemetry-ruler\n 1.2.0\n 1.2.0\n 1.2.0\n   whizard-telemetry\n 1.2.2\n 1.3.0\n 2.0.0\n   wiztelemetry-bpfconductor\n Not included\n Not included\n 1.0.3\n   wiztelemetry-tracing\n Not included\n Not included\n 1.0.3\n       ","href":"/en/v4.2.0/03-installation-and-upgrade/03-upgrade-kubesphere/04-upgrade-kubephere-from-v4.1/","isSection":null,"linkkey":null,"title":"Offline Upgrade from v4.1.x to v4.2.0"},{"content":"This section explains how to uninstall KubeSphere Enterprise.\n ","href":"/en/v4.2.0/03-installation-and-upgrade/04-uninstall-kubesphere/","isSection":null,"linkkey":null,"title":"Uninstall KubeSphere Enterprise"},{"content":"This section explains how to add and remove cluster nodes in KubeSphere Enterprise.\n ","href":"/en/v4.2.0/03-installation-and-upgrade/05-add-and-delete-cluster-nodes/","isSection":null,"linkkey":null,"title":"Add and Remove Cluster Nodes"},{"content":" Configuration for Upgrade Job     Paremeter Default Value Description     upgrade.enabled\n true\n bool - Enable upgrade component or not.\n   upgrade.image.registry\n \u0026#34;\u0026#34;\n string - Set the image registry for the upgrade job.\n   upgrade.image.repository\n kse/ks-upgrade\n string - Set the image repository for the upgrade job.\n   upgrade.image.tag\n \u0026#34;\u0026#34;\n string - Set the image tag for the upgrade job.\n   upgrade.image.pullPolicy\n Always\n string - Set the image pull policy for the upgrade job.\n   upgrade.persistenceVolume.name\n ks-upgrade\n string - Set the persistent volume name for the upgrade job.\n   upgrade.persistenceVolume.storageClassName\n \u0026#34;\u0026#34;\n string - Set the storage class name for the upgrade job.\n   upgrade.persistenceVolume.accessMode\n ReadWriteOnce\n string - Set the access mode of the persistent volume for the upgrade job.\n   upgrade.persistenceVolume.size\n 5Gi\n string - Set the size of the persistent volume for the upgrade job.\n   upgrade.resources.limit.cpu\n 1\n string - Set the CPU resource limit for the upgrade job.\n   upgrade.resources.limit.memory\n 1024Mi\n string - Set the Memory resource limit for the upgrade job.\n   upgrade.resources.requests.cpu\n 20m\n string - Set the CPU resource request for the upgrade job.\n   upgrade.resources.requests.memory\n 100Mi\n string - Set the Memory resource request for the upgrade job.\n      Configuration for Upgrade Application     Paremeter Default Value Description     upgrade.config.storage.local.path\n /tmp/ks-upgrade\n string - Set the local directory used by the upgrade job to store backup data.\n   upgrade.config.storage.s3.endpoint\n \u0026#34;\u0026#34;\n string - Set the endpoint for the S3-compatible object storage service used by the upgrade job to store backup data.\n   upgrade.config.storage.s3.region\n \u0026#34;\u0026#34;\n string - Set the data storage region of the S3 service.\n   upgrade.config.storage.s3.disableSSL\n false\n bool - Disable SSL for S3 or not.\n   upgrade.config.storage.s3.forcePathStyle\n false\n bool - Set the S3 client to use path-style addressing for buckets or not.\n   upgrade.config.storage.s3.accessKeyID\n \u0026#34;\u0026#34;\n string - Set the access key ID for S3 service account, required if credentials are not used.\n   upgrade.config.storage.s3.secretAccessKey\n \u0026#34;\u0026#34;\n string - Set the secret access key for S3 service account, required if credentials are not used.\n   upgrade.config.storage.s3.sessionToken\n \u0026#34;\u0026#34;\n string - Set the session token for accessing the S3 service.\n   upgrade.config.storage.s3.bucket\n \u0026#34;\u0026#34;\n string - Set the S3 bucket name.\n   upgrade.config.download.globalRegistryUrl\n oci://hub.kubesphere.com.cn/kse-extensions\n string - Set the repository URL for extensions.\n   upgrade.config.download.file\n -\n System reserved configuration.\n   upgrade.config.download.http.timeout\n 20\n int64 - Set the timeout for downloading extensions.\n   upgrade.config.download.http.caBundle\n \u0026#34;\u0026#34;\n string - Set the base64 string of the self-signed certificates for the extension repository, with multiple self-signed certificates combined into one base64 string.\n   upgrade.config.download.http.insecureSkipVerify\n true\n bool - Skip TLS verification for the extension repository or not.\n   upgrade.config.download.oci\n -\n System reserved configuration.\n   upgrade.config.skipValidator\n false\n bool - Skip version validation for the upgrade job or not. The validation is specific to the KubeSphere version.\n      Configuration for Upgrade Components     Paremeter Default Value Description     upgrade.config.jobs.$ID.enabled\n false\n bool - Set whether to enable the component $ID to enable the upgrade job.\n   upgrade.config.jobs.$ID.priority\n 0\n int - Set the upgrade priority of the component.\n   upgrade.config.jobs.$ID.extensionRef.name\n \u0026#34;\u0026#34;\n string - Set the extension name.\n   upgrade.config.jobs.$ID.extensionRef.version\n \u0026#34;\u0026#34;\n string - Set the extension version.\n      ","href":"/en/v4.2.0/03-installation-and-upgrade/03-upgrade-kubesphere/05-appendix-ks-core/","isSection":null,"linkkey":null,"title":"Appendix 1: ks-core Helm Chart Upgrade Parameters"},{"content":" KubeSphere Helm Chart Parameters Global Configuration     Name Description Value     global.imageRegistry\n Global Docker image registry\n registry.cn-beijing.aliyuncs.com\n   global.tag\n Global Docker image tag\n \u0026#34;\u0026#34;\n   global.imagePullSecrets\n Global Docker registry secret names as an array\n []\n    Common Parameters     Name Description Value     nameOverride\n String to partially override common.names.fullname\n \u0026#34;\u0026#34;\n   fullnameOverride\n String to fully override common.names.fullname\n \u0026#34;\u0026#34;\n   commonLabels\n Labels to add to all deployed objects\n {}\n   commonAnnotations\n Annotations to add to all deployed objects\n {}\n    Multi-cluster Configuration     Name Description Value     multicluster.role\n Multi-cluster role (host/member)\n \u0026#34;\u0026#34;\n   multicluster.hostClusterName\n Host cluster name\n \u0026#34;\u0026#34;\n    Portal Configuration     Name Description Value     portal.hostname\n The public domain name or IP address to access the ks-console service\n ks-console.kubesphere-system.svc\n   portal.http.port\n The HTTP port exposed by the ks-console service through the public portal\n 30880\n    S3 Storage Configuration     Name Description Value     s3.endpoint\n S3 endpoint URL\n \u0026#34;\u0026#34;\n   s3.region\n S3 region\n us-east-1\n   s3.disableSSL\n Disable SSL for S3\n true\n   s3.forcePathStyle\n Force path style for S3\n true\n   s3.accessKeyID\n S3 access key ID\n admin\n   s3.secretAccessKey\n S3 secret access key\n admin\n   s3.bucket\n S3 bucket name\n uploads\n    Authentication Configuration     Name Description Value     authentication.authenticateRateLimiterMaxTries\n Maximum authentication attempts\n 10\n   authentication.authenticationRateLimiterDuration\n Rate limiter duration\n 10m0s\n   authentication.loginHistoryRetentionPeriod\n Login history retention period\n 168h\n   authentication.enableMultiLogin\n Enable multi-login\n true\n   authentication.maxInactivityDuration\n Maximum inactivity duration\n 0s\n   authentication.adminPassword\n Admin password\n \u0026#34;\u0026#34;\n   authentication.issuer.maximumClockSkew\n Maximum clock skew for JWT\n 10s\n   authentication.issuer.jwtSecret\n JWT secret\n \u0026#34;\u0026#34;\n   authentication.issuer.accessTokenMaxAge\n Access token maximum age\n 2h\n   authentication.issuer.accessTokenInactivityTimeout\n Access token inactivity timeout\n 30m\n    Security Configuration     Name Description Value     security.passwordPolicy.expireEnabled\n Enable password expiration\n false\n   security.passwordPolicy.maxAgeDays\n Maximum password age in days\n 90\n   security.passwordPolicy.reminderDays\n Days before password expiration to start reminding\n 7\n   security.passwordPolicy.forceChangeOnExpire\n Force users to change password when expired\n false\n    Experimental Features     Name Description Value     experimental.validationDirective\n Validation directive (Strict/Ignore/Warn)\n \u0026#34;\u0026#34;\n   experimental.maintenance.enabled\n Enable maintenance mode\n false\n   experimental.maintenance.platformMaintenance\n Enable platform maintenance\n false\n   experimental.maintenance.description\n Maintenance description\n \u0026#34;\u0026#34;\n   experimental.openAPI.enabled\n Enable OpenAPI\n false\n   experimental.hnc.enabled\n Enable HNC (Hierarchical Namespace Controller)\n false\n    Auditing Configuration     Name Description Value     auditing.enable\n Enable auditing\n false\n   auditing.auditLevel\n Audit level (Metadata/Request/RequestResponse)\n Metadata\n   auditing.logOptions.path\n Audit log path\n /etc/audit/audit.log\n   auditing.logOptions.maxAge\n Maximum age of audit logs\n 7\n   auditing.logOptions.maxBackups\n Maximum number of audit log backups\n 10\n   auditing.logOptions.maxSize\n Maximum size of audit log files\n 100\n    Service Account Configuration     Name Description Value     serviceAccount.create\n Create service account\n true\n   serviceAccount.annotations\n Service account annotations\n {}\n   serviceAccount.name\n Service account name\n kubesphere\n    Pod Configuration     Name Description Value     tolerations\n Pod tolerations\n []\n   affinity\n Pod affinity configuration\n {}\n   nodeSelector\n Node selector for pod assignment\n {}\n   internalTLS\n Enable TLS communication between all components\n false\n    API Server Configuration     Name Description Value     apiserver.image.registry\n API server image registry\n \u0026#34;\u0026#34;\n   apiserver.image.repository\n API server image repository\n kse/ks-apiserver\n   apiserver.image.tag\n API server image tag\n \u0026#34;\u0026#34;\n   apiserver.image.digest\n API server image digest\n \u0026#34;\u0026#34;\n   apiserver.image.pullPolicy\n API server image pull policy\n IfNotPresent\n   apiserver.containerPorts\n List of container ports to enable in the ks-apiserver container\n []\n   apiserver.resources.limits\n The resource limits for the ks-apiserver containers\n {}\n   apiserver.resources.requests\n The requested resources for the ks-apiserver containers\n {}\n   apiserver.command\n Override default container command\n []\n   apiserver.extraEnvVars\n Array with extra environment variables to add to ks-apiserver\n []\n   apiserver.extraVolumeMounts\n Extra list of additional volumeMounts for the ks-apiserver container(s)\n []\n   apiserver.extraVolumes\n Extra list of additional volumes for the ks-apiserver pod(s)\n []\n   apiserver.hardAntiAffinity\n Whether the ks-apiserver pods should be forced to run on separate nodes\n false\n    Console Configuration     Name Description Value     console.image.registry\n Console image registry\n \u0026#34;\u0026#34;\n   console.image.repository\n Console image repository\n kse/ks-console\n   console.image.tag\n Console image tag\n \u0026#34;\u0026#34;\n   console.image.digest\n Console image digest\n \u0026#34;\u0026#34;\n   console.image.pullPolicy\n Console image pull policy\n IfNotPresent\n   console.config.enableKubeConfig\n Enable kubeconfig in console\n true\n   console.config.enableNodeListTerminal\n Enable node list terminal in console\n true\n   console.containerPorts\n List of container ports to enable in the ks-console container\n []\n   console.nodePort\n Node port for console service\n 30880\n   console.resources.limits\n The resource limits for the ks-console containers\n {}\n   console.resources.requests\n The requested resources for the ks-console containers\n {}\n   console.command\n Override default container command\n []\n   console.extraEnvVars\n Array with extra environment variables to add to ks-console\n []\n   console.extraVolumeMounts\n Extra list of additional volumeMounts for the ks-console container(s)\n []\n   console.extraVolumes\n Extra list of additional volumes for the ks-console pod(s)\n []\n   console.hardAntiAffinity\n Whether the ks-console pods should be forced to run on separate nodes\n false\n    Controller Configuration     Name Description Value     controller.image.registry\n Controller image registry\n \u0026#34;\u0026#34;\n   controller.image.repository\n Controller image repository\n kse/ks-controller-manager\n   controller.image.tag\n Controller image tag\n \u0026#34;\u0026#34;\n   controller.image.digest\n Controller image digest\n \u0026#34;\u0026#34;\n   controller.image.pullPolicy\n Controller image pull policy\n IfNotPresent\n   controller.containerPorts\n List of container ports to enable in the ks-controller-manager container\n []\n   controller.resources.limits\n The resource limits for the ks-controller-manager containers\n {}\n   controller.resources.requests\n The requested resources for the ks-controller-manager containers\n {}\n   controller.command\n Override default container command\n []\n   controller.extraEnvVars\n Array with extra environment variables to add to ks-controller-manager\n []\n   controller.extraVolumeMounts\n Extra list of additional volumeMounts for the ks-controller-manager container(s)\n []\n   controller.extraVolumes\n Extra list of additional volumes for the ks-controller-manager pod(s)\n []\n   controller.hardAntiAffinity\n Whether the ks-controller-manager pods should be forced to run on separate nodes\n false\n    Agent Configuration     Name Description Value     agent.replicaCount\n Number of agent replicas\n 1\n    Helm Executor Configuration     Name Description Value     helmExecutor.timeout\n Helm executor timeout\n 10m\n   helmExecutor.historyMax\n Maximum helm history\n 2\n   helmExecutor.jobTTLAfterFinished\n Job TTL after finished\n 0s\n   helmExecutor.image.registry\n Helm executor image registry\n \u0026#34;\u0026#34;\n   helmExecutor.image.repository\n Helm executor image repository\n kubesphereio/kubectl\n   helmExecutor.image.tag\n Helm executor image tag\n v1.33.1\n   helmExecutor.image.pullPolicy\n Helm executor image pull policy\n IfNotPresent\n   helmExecutor.resources.limits\n Resource limits for helm executor\n {}\n   helmExecutor.resources.requests\n Resource requests for helm executor\n {}\n   helmExecutor.affinity\n Affinity configuration for helm executor\n {}\n    Composed App Configuration     Name Description Value     composedApp.appSelector\n Selector to filter k8s applications to reconcile\n \u0026#34;\u0026#34;\n    Kubectl Configuration     Name Description Value     kubectl.image.registry\n Kubectl image registry\n \u0026#34;\u0026#34;\n   kubectl.image.repository\n Kubectl image repository\n kubesphereio/kubectl\n   kubectl.image.tag\n Kubectl image tag\n v1.33.1\n   kubectl.image.pullPolicy\n Kubectl image pull policy\n IfNotPresent\n    Ingress Configuration     Name Description Value     ingress.enabled\n Enable ingress\n false\n   ingress.ingressClassName\n Ingress class name\n \u0026#34;\u0026#34;\n   ingress.tls.enabled\n Enable TLS\n true\n   ingress.tls.source\n TLS source (generation/importation/letsEncrypt)\n generation\n   ingress.tls.secretName\n TLS secret name\n kubesphere-tls-certs\n    Let’s Encrypt Configuration     Name Description Value     letsEncrypt.environment\n Let’s Encrypt environment (production/staging)\n production\n    Cert Manager Configuration     Name Description Value     certmanager.duration\n Certificate duration\n 2160h\n   certmanager.renewBefore\n Certificate renewal before expiration\n 360h\n    Terminal Configuration     Name Description Value     terminal.kubectl.enabled\n Enable kubectl terminal\n true\n   terminal.kubectl.image.registry\n Kubectl terminal image registry\n \u0026#34;\u0026#34;\n   terminal.kubectl.image.repository\n Kubectl terminal image repository\n kubesphereio/kubectl\n   terminal.kubectl.image.tag\n Kubectl terminal image tag\n v1.33.1\n   terminal.kubectl.image.pullPolicy\n Kubectl terminal image pull policy\n IfNotPresent\n   terminal.node.enabled\n Enable node terminal\n true\n   terminal.node.image.registry\n Node terminal image registry\n \u0026#34;\u0026#34;\n   terminal.node.image.repository\n Node terminal image repository\n kubesphereio/kubectl\n   terminal.node.image.tag\n Node terminal image tag\n v1.33.1\n   terminal.node.image.pullPolicy\n Node terminal image pull policy\n IfNotPresent\n   terminal.pod.enabled\n Enable pod terminal\n true\n   terminal.pod.uploadFileLimit\n Upload file limit for pod terminal\n 100Mi\n   terminal.pod.uploadFileEnabled\n Enable file upload in pod terminal\n true\n   terminal.pod.downloadFileEnabled\n Enable file download in pod terminal\n true\n    Cloud Configuration     Name Description Value     cloud.enabled\n Enable cloud features\n false\n   cloud.env\n Cloud environment\n kubesphere.cloud\n   cloud.customEnv\n Custom cloud environment configuration\n {}\n    Extension Configuration     Name Description Value     extension.imageRegistry\n Extension image registry\n \u0026#34;\u0026#34;\n   extension.nodeSelector\n Node selector for extensions\n {}\n   extension.ingress.ingressClassName\n Extension ingress class name\n \u0026#34;\u0026#34;\n   extension.ingress.domainSuffix\n Domain suffix for extension ingresses\n \u0026#34;\u0026#34;\n   extension.ingress.httpPort\n HTTP port for extension ingress\n 80\n   extension.ingress.httpsPort\n HTTPS port for extension ingress\n 443\n    Upgrade Configuration     Name Description Value     upgrade.enabled\n Enable upgrade\n false\n   upgrade.image.registry\n Upgrade image registry\n \u0026#34;\u0026#34;\n   upgrade.image.repository\n Upgrade image repository\n kse/ks-upgrade\n   upgrade.image.tag\n Upgrade image tag\n \u0026#34;\u0026#34;\n   upgrade.image.pullPolicy\n Upgrade image pull policy\n IfNotPresent\n   upgrade.persistenceVolume.name\n Upgrade persistence volume name\n ks-upgrade\n   upgrade.persistenceVolume.storageClassName\n Upgrade storage class name\n \u0026#34;\u0026#34;\n   upgrade.persistenceVolume.accessMode\n Upgrade access mode\n ReadWriteOnce\n   upgrade.persistenceVolume.size\n Upgrade volume size\n 5Gi\n   upgrade.config\n Upgrade configuration\n {}\n    High Availability Configuration     Name Description Value     ha.enabled\n Enable high availability\n false\n    Redis Configuration     Name Description Value     redis.port\n Redis port\n 6379\n   redis.replicaCount\n Redis replica count\n 1\n   redis.image.registry\n Redis image registry\n \u0026#34;\u0026#34;\n   redis.image.repository\n Redis image repository\n kubesphereio/redis\n   redis.image.digest\n Redis image digest\n \u0026#34;\u0026#34;\n   redis.image.tag\n Redis image tag\n 7.2.4-alpine\n   redis.image.pullPolicy\n Redis image pull policy\n IfNotPresent\n   redis.persistentVolume.enabled\n Enable Redis persistent volume\n true\n   redis.persistentVolume.size\n Redis persistent volume size\n 2Gi\n    Redis HA Configuration     Name Description Value     redisHA.enabled\n Enable Redis HA\n false\n   redisHA.redis.port\n Redis HA port\n 6379\n   redisHA.image.registry\n Redis HA image registry\n \u0026#34;\u0026#34;\n   redisHA.image.repository\n Redis HA image repository\n kubesphereio/redis\n   redisHA.image.tag\n Redis HA image tag\n 7.2.8-alpine\n   redisHA.image.pullPolicy\n Redis HA image pull policy\n IfNotPresent\n   redisHA.persistentVolume.enabled\n Enable Redis HA persistent volume\n true\n   redisHA.persistentVolume.size\n Redis HA persistent volume size\n 2Gi\n   redisHA.auth\n Enable Redis HA authentication\n true\n   redisHA.existingSecret\n Redis HA existing secret\n redis-secret\n   redisHA.tolerations\n Redis HA tolerations\n []\n   redisHA.hardAntiAffinity\n Redis HA hard anti-affinity\n false\n   redisHA.additionalAffinities\n Redis HA additional affinities\n {}\n   redisHA.haproxy.servicePort\n HAProxy service port\n 6379\n   redisHA.haproxy.containerPort\n HAProxy container port\n 6379\n   redisHA.haproxy.image.registry\n HAProxy image registry\n \u0026#34;\u0026#34;\n   redisHA.haproxy.image.repository\n HAProxy image repository\n kubesphereio/haproxy\n   redisHA.haproxy.image.tag\n HAProxy image tag\n 3.0.8-alpine\n   redisHA.haproxy.image.digest\n HAProxy image digest\n \u0026#34;\u0026#34;\n   redisHA.haproxy.image.pullPolicy\n HAProxy image pull policy\n IfNotPresent\n   redisHA.haproxy.hardAntiAffinity\n HAProxy hard anti-affinity\n false\n   redisHA.haproxy.additionalAffinities\n HAProxy additional affinities\n {}\n    KubeSphere CRDs Configuration     Name Description Value     ksCRDs.kubectl.image.registry\n Kubectl image registry for CRDs\n \u0026#34;\u0026#34;\n   ksCRDs.kubectl.image.repository\n Kubectl image repository for CRDs\n kubesphereio/kubectl\n   ksCRDs.kubectl.image.tag\n Kubectl image tag for CRDs\n v1.33.1\n   ksCRDs.kubectl.image.pullPolicy\n Kubectl image pull policy for CRDs\n IfNotPresent\n    KSE Extension Repository Configuration     Name Description Value     kseExtensionRepository.enabled\n Enable KSE extension repository\n true\n   kseExtensionRepository.image.registry\n Extension repository image registry\n \u0026#34;\u0026#34;\n   kseExtensionRepository.image.repository\n Extension repository image repository\n kse/extensions-museum\n   kseExtensionRepository.image.tag\n Extension repository image tag\n v11.0.0\n   kseExtensionRepository.image.pullPolicy\n Extension repository image pull policy\n IfNotPresent\n    KubeSphere Console Embed Configuration     Name Description Value     ksConsoleEmbed.image.repository\n Console embed image repository\n kse/ks-console-embed\n   ksConsoleEmbed.image.tag\n Console embed image tag\n \u0026#34;\u0026#34;\n   ksConsoleEmbed.image.pullPolicy\n Console embed image pull policy\n IfNotPresent\n    Application Configuration     Name Description Value     application.builtinRepo.enabled\n Enable built-in repository\n true\n      TLS Configuration  Select SSL Configuration\nKubeSphere security configuration includes Ingress SSL Configuration and Internal SSL Configuration. The Ingress SSL Configuration supports three modes by default to enable SSL/TLS for secure access.\n    Ingress SSL Configuration\n    Configuration Helm Chart Option Cert-manager Required     KubeSphere Generated TLS Certificates\n ingress.tls.source=generation\n No\n   Let’s Encrypt\n ingress.tls.source=letsEncrypt\n Yes\n   Import Existing Certificates\n ingress.tls.source=importation\n No\n      KubeSphere Generated TLS Certificates: Supports both cert-manager and Helm methods.\n  If cert-manager is already installed in the Kubernetes cluster, it is preferred to use cert-manager to generate certificates. KubeSphere uses cert-manager to issue and maintain certificates. KubeSphere generates its CA certificate, signs a certificate using that CA, and then manages the certificate with cert-manager.\n  If cert-manager is not installed, Helm is used to generate certificates. During the installation process with Helm, KubeSphere generates CA and TLS certificates based on the configured hostname. In this option, certificates do not support automatic expiration rotation.\n     Let’s Encrypt\nWhen using the Let’s Encrypt option, cert-manager must be utilized. In this scenario, cert-manager combines with a special issuer for Let’s Encrypt that performs all actions (including request and validation) necessary for getting a Let’s Encrypt issued cert. This configuration uses HTTP validation (HTTP-01), so the load balancer must have a public DNS record and be accessible from the internet.\n   Import Existing Certificates\nThis option allows you to bring your own public- or private-CA signed certificate. KubeSphere will use that certificate to secure websocket and HTTPS traffic. In this case, you must upload this certificate (and associated key) as PEM-encoded files with the name tls.crt and tls.key. If you are using a private CA, you must also upload that certificate. This is due to the fact that this private CA may not be trusted by your nodes.\n        Internal SSL Configuration\nAfter enabling internal SSL configuration, both Console UI and Apiserver in KubeSphere will use HTTPS. This configuration inherently supports cert-manager and helm generated certificates. When cert-manager is already installed in the Kubernetes cluster, it is preferred to use cert-manager to generate/manage certificates, and the DNS for certificates defaults to Console UI and Apiserver’s Service DNS within the Kubernetes cluster.\n     Configuration Helm Chart Option Cert-manager Required     Enable Internal SSL\n internalTLS=true\n No\n           Install cert-manager\nIf you are using your own certificate files (ingress.tls.source=importation), you can skip this step.\n Only when using KubeSphere-generated certificates (ingress.tls.source=generation) or Let’s Encrypt issued certificates (ingress.tls.source=letsEncrypt), you need to install cert-manager.\n # Add Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io # Update local Helm Chart repository cache helm repo update # Install cert-manager Helm Chart helm install cert-manager jetstack/cert-manager -n cert-manager --create-namespace --set prometheus.enabled=false # Or kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/\u0026lt;VERSION\u0026gt;/cert-manager.yaml   After installing cert-manager, check the running pods in the cert-manager namespace to verify that it has been deployed correctly:\n kubectl get pods --namespace cert-manager       Based on your selected certificate option, enable SSL configuration for KubeSphere using Helm.\n Enable Ingress SSL Configuration\n  KubeSphere Generated Certificates\nhelm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version \\ --set ingress.enabled=true \\ --set hostname=kubesphere.my.org     Let’s Encrypt\nThis option uses cert-manager to automatically request and renew Let’s Encrypt certificates. Let’s Encrypt is free and a trusted CA, so it can provide valid certificates.\n helm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version \\ --set hostname=kubesphere.my.org \\ --set ingress.enabled=true \\ --set ingress.tls.source=letsEncrypt \\ --set letsEncrypt.email=me@example.org     Import External Certificates\n# Import external certificates kubectl create secret tls tls-ks-core-ingress --cert=tls.crt --key=tls.key -n kubesphere-system # Install KubeSphere helm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version \\ --set ingress.enabled=true \\ --set hostname=kubesphere.my.org \\ --set ingress.tls.source=importation          Enable Internal SSL Configuration.\nhelm upgrade --install -n kubesphere-system --create-namespace ks-core $chart --version $version \\ --set internalTLS=true           Configure the ratelimit Limiter Once the limiter is enabled, it will independently limit requests for all users in KubeSphere, primarily supporting the following two methods:\n   Setting a rate limit for all users in KubeSphere, without support for setting individual rate limits for each user at the moment;\n  Setting a rate limit independently for each ServiceAccount in KubeSphere.\n   Enable the Limiter Enabling the limiter means setting a rate limit for all users in KubeSphere.\n  Modify the kubesphere-system configmap.\nkubectl -n kubesphere-system edit cm kubesphere-system   Add the following content:\n rateLimit: enable: true # Enable the limiter driver: memory # Memory mode QPS: 40.0 # Token recovery rate burst: 80 # Token bucket capacity       Restart the ks-apiserver.\nkubectl -n kubesphere-system rollout restart deploy ks-apiserver      Set the ServiceAccount Limiter Before setting, you need to enable the limiter as in the previous step. Then execute the following command to set the rate limit for ServiceAccount.\n kubectl -n \u0026lt;Namespace\u0026gt; patch serviceaccounts.kubesphere.io \u0026lt;ServiceAccount\u0026gt; --type merge -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;annotations\u0026#34;: {\u0026#34;kubesphere.io/ratelimiter-qps\u0026#34;: \u0026#34;20.0\u0026#34;, \u0026#34;kubesphere.io/ratelimiter-burst\u0026#34;: \u0026#34;40\u0026#34;}}}\u0026#39;   Parameter Description     Option Default Value Description     rateLimit.enable\n false\n bool - Enable the limiter.\n   rateLimit.driver\n memory\n string - Limiter storage type, options: \u0026#34;memory\u0026#34;.\n   rateLimit.QPS\n 5.0\n float32 - Number of tokens recovered per second in the limiter token bucket algorithm.\n   rateLimit.burst\n 10\n int - Maximum capacity of the token bucket in the limiter token bucket algorithm.\n        Note     The recommended QPS for token recovery rate should be half the burst capacity.\n      ","href":"/en/v4.2.0/03-installation-and-upgrade/02-install-kubesphere/05-appendix/","isSection":null,"linkkey":null,"title":"Appendix: Advanced Configuration of KubeSphere Core"},{"content":" This section describes how to configure Network File System (NFS) for the KubeSphere cluster in a production environment.\n     Note     NFS is incompatible with some applications (such as Prometheus), which may cause pod creation failure. If you do need to use NFS in a production environment, make sure you understand the risks or consult KubeSphere technical support at support@kubesphere.cloud.\n    Build NFS Server Before configuring NFS for KubeSphere, you need to set up an NFS server first. If you already have an NFS server available, you can skip this step. The following uses the Ubuntu operating system and NFS Kernel Server as an example to introduce how to build an NFS server. For specific operations on other NFS servers and other operating systems, see the user guides for the NFS servers and operating systems.\n Prerequisites You need to prepare a Linux server (the Ubuntu operating system is used as an example below), which must be connected to the KubeSphere cluster node network.\n  Steps  Log in to the server used to build the NFS server and execute the following command to install the NFS Kernel Server:\nsudo apt update   sudo apt install nfs-kernel-server       Execute the following command to create a directory for use by KubeSphere (replace \u0026lt;directory\u0026gt; with the actual directory path, such as /mnt/demo):\nsudo mkdir -p \u0026lt;directory\u0026gt;       Execute the following command to remove access restrictions on the directory (replace \u0026lt;directory\u0026gt; with the actual directory path, such as /mnt/demo):\nsudo chown nobody:nogroup \u0026lt;directory\u0026gt;   sudo chmod 777 \u0026lt;directory\u0026gt;       Execute the following command to edit the configuration file of NFS Kernel Server:\nsudo vi /etc/exports       Add the KubeSphere cluster node information to the file to allow the server to access the NFS server, and save the file:\n\u0026lt;directory\u0026gt; \u0026lt;IP address\u0026gt;(rw,sync,no_subtree_check)   Replace the following parameters with actual values:\n     Parameter Description     \u0026lt;directory\u0026gt;\n The directory used by KubeSphere, for example /mnt/demo.\n   \u0026lt;IP address\u0026gt;\n The IP address of the KubeSphere cluster node, for example 192.168.0.2.\n    If there are multiple KubeSphere cluster nodes, please set multiple configuration entries. You can also set \u0026lt;IP address\u0026gt; to a network segment so that all servers in the network segment can access the NFS server, for example 192.168.0.0/24.\n     Execute the following command to enable directory sharing:\nsudo exportfs -a       Execute the following command to restart the NFS server to make the configuration take effect:\nsudo systemctl restart nfs-kernel-server           Configure Cluster Nodes After the NFS server is built, you need to install the client tool on the KubeSphere cluster nodes and create a configuration file for subsequent installation of KubeSphere. After KubeSphere is installed, the NFS server specified in the configuration file will be used as persistent storage. The following uses the Ubuntu operating system to install NFS Common as an example to introduce how to install the NFS client and create a configuration file. For specific operations on other NFS clients and other operating systems, see the user guides for the NFS clients and operating systems.\n Prerequisites You should set up an NFS server. For more information, see Build NFS Server.\n  Steps  Log in to all KubeSphere cluster nodes and execute the following command to install nfs-common:\nsudo apt update   sudo apt install nfs-common       Log in to the cluster node used to perform the KubeSphere installation and execute the following command to create the configuration file for NFS storage plugin:\nvi nfs-client.yaml       Add the following information to the configuration file and save the file for subsequent installation of KubeSphere:\nnfs: server: \u0026#34;\u0026lt;IP address\u0026gt;\u0026#34; path: \u0026#34;\u0026lt;directory\u0026gt;\u0026#34; storageClass: defaultClass: true   Replace the following parameters with actual values:\n     Parameter Description     \u0026lt;IP address\u0026gt;\n IP address of the NFS server.\n   \u0026lt;directory\u0026gt;\n The directory of NFS server used by KubeSphere.\n    The above configuration file only contains the parameters that must be set. To set other parameters, see NFS Client Configuration.\n         ","href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/05-configure-nfs/","isSection":null,"linkkey":null,"title":"Configure NFS"},{"content":"KubeSphere supports numerous third-party open source storage systems, including but not limited to:\n   Ceph CSI   GlusterFS   OpenEBS   Longhorn    KubeSphere does not provide installation packages for third-party open source storage systems. Please download and install them from the official websites of the storage systems.\n ","href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/07-configure-opensource-storage/","isSection":null,"linkkey":null,"title":"Configure Open Source Storage Systems"},{"content":"","href":"/en/v4.2.0/","isSection":[{"children":[{"children":[{"title":"Environment Requirements","url":"03-installation-and-upgrade/01-preparations/01-supported-k8s/"},{"title":"Configure High Availability","url":"03-installation-and-upgrade/01-preparations/02-configure-high-availability/"},{"title":"Configure Persistent Storage","url":"03-installation-and-upgrade/01-preparations/04-configure-external-persistent-storage/"}],"title":null}],"icon":"icon-note_tree_duotone","title":"Prepare"},{"children":[{"children":[{"title":"Install KubeSphere Enterprise","url":"03-installation-and-upgrade/02-install-kubesphere/01-online-install-kubernetes-and-kubesphere/"},{"title":"Active KubeSphere Enterprise","url":"03-installation-and-upgrade/02-install-kubesphere/03-activate-kse/"},{"title":"Advanced Configuration","url":"03-installation-and-upgrade/02-install-kubesphere/05-appendix/"}],"title":null}],"icon":"icon-gamepad_2_duotone","title":"Install"},{"children":[{"children":[{"title":"Online Upgrade","url":"03-installation-and-upgrade/03-upgrade-kubesphere/03-online-upgrade-kubephere-from-4.1.x/"},{"title":"Offline Upgrade","url":"03-installation-and-upgrade/03-upgrade-kubesphere/04-upgrade-kubephere-from-v4.1/"},{"title":"Upgrade Parameters","url":"03-installation-and-upgrade/03-upgrade-kubesphere/05-appendix-ks-core/"}],"title":null}],"icon":"icon-simulation_duotone","title":"Upgrade"},{"children":[{"children":[{"title":"Uninstall KubeSphere Enterprise Only","url":"03-installation-and-upgrade/04-uninstall-kubesphere/01-uninstall-kubesphere-only/"},{"title":"Uninstall Kubernetes and KubeSphere Enterprise","url":"03-installation-and-upgrade/04-uninstall-kubesphere/02-uninstall-kubernetes-and-kubesphere/"}],"title":null}],"icon":"icon-apps_2_duotone","title":"Uninstall"},{"children":[{"children":[{"title":"Add Cluster Nodes","url":"03-installation-and-upgrade//05-add-and-delete-cluster-nodes/01-add-cluster-nodes/"},{"title":"Remove Cluster Nodes","url":"03-installation-and-upgrade/05-add-and-delete-cluster-nodes/02-delete-cluster-nodes/"}],"title":null}],"icon":"icon-operation_manage_duotone","title":"Manage"}],"linkkey":null,"title":"KubeSphere Enterprise"},{"content":"","firstChild":{"href":"/en/categories/","title":"Categories"},"href":"/en/categories/","isSection":null,"linkkey":null,"title":"KubeSphere Enterprise"},{"content":"","firstChild":{"href":"/en/v4.2.0/03-installation-and-upgrade/01-preparations/01-supported-k8s/","title":"Environment Requirements"},"href":"/en/","isSection":null,"linkkey":null,"title":"KubeSphere Enterprise"},{"content":"","firstChild":{"href":"/en/tags/","title":"Tags"},"href":"/en/tags/","isSection":null,"linkkey":null,"title":"KubeSphere Enterprise"},{"content":"","firstChild":{"href":"/en/v4.2.0/search/","title":"搜索功能"},"href":"/en/v4.2.0/search/","isSection":null,"linkkey":null,"title":"搜索功能"}]